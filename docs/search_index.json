[
["index.html", "DataFest2021 Example Project What is this? Project summary Authors", " DataFest2021 Example Project December 2020 What is this? The materials in this repository serve as a presenter-facing example of one possible project workflow for DataFest2021. Presenters should feel free to use, modify, or wrangle this material into a form that suits their preferences. Or, they should feel free to ignore the material entirely if there is a particular direction they have in mind for their session. Untimately, each presenter should create their own R or Python script that they will work through during their session (source code, Rmarkdown, or notebook formats are all fine). The materials should not be shared with participants ahead of time, but we will upload materials to the main IQSS/datafest Github repo at the end of each day, so that participants attending only a subset of days can orientate themselves to what has been presented previously. Project summary The ‘research project’ focuses on investigating COVID-19 case rates over time and space. The goal is for participants to learn about best practices for handling and using data throughout the data life-cycle, which includes: data acquisition and cleaning, data visualization and analysis, as well as data archiving and dissemination. They will learn how to programmatically extract COVID-19 data from online sources using APIs, wrangle those data into a clean state, visualize the data temporally and spatially, analyze the data using a variety of statistical models, and finally archive the project replication files (code and data) into an online data repository (Harvard Dataverse). Authors The content in this repo was created by Steven Worthington (IQSS) and Jinjie Liu (IQSS). "],
["project-overview.html", "Project overview", " Project overview Over the next few days, we’ll be engaged in a research project investigating COVID-19 case rates over time and space. The goal is to learn about best practices for handling and using data throughout the data life-cycle, which includes: data acquisition and cleaning, data visualization and analysis, as well as data archiving and dissemination. We’ll learn how to programmatically extract COVID-19 data from online sources, wrangle those data into a clean state, visualize the data temporally and spatially, analyze the data using a variety of statistical models, and finally archive the project replication files (code and data) into an online data repository. "],
["r-setup.html", "R setup", " R setup In early January, we’ll create an installation guide for R and RStudio (based on this one: https://iqss.github.io/dss-workshops/Rinstall.html), together with instructions for installing packages, so that all participants can arrive with working R environments. For now, we just need to install and load the necessary packages for the current material: ipak &lt;- function(pkg){ new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, &quot;Package&quot;])] if(length(new.pkg)) install.packages(new.pkg, dependencies = TRUE) sapply(pkg, require, character.only = TRUE) } packages &lt;- c(&quot;viridis&quot;, &quot;glmmTMB&quot;, &quot;effects&quot;, &quot;dataverse&quot;, &quot;sf&quot;, &quot;remotes&quot;, &quot;leaflet&quot;, &quot;mapview&quot;, &quot;htmltools&quot;, &quot;htmlwidgets&quot;, &quot;tigris&quot;, &quot;lubridate&quot;, &quot;DHARMa&quot;, &quot;tidycensus&quot;, &quot;tidyverse&quot;, &quot;tidymodels&quot;) ipak(packages) ## viridis glmmTMB effects dataverse sf remotes ## TRUE TRUE TRUE TRUE TRUE TRUE ## leaflet mapview htmltools htmlwidgets tigris lubridate ## TRUE TRUE TRUE TRUE TRUE TRUE ## DHARMa tidycensus tidyverse tidymodels ## TRUE TRUE TRUE TRUE # mapview may need to be installed from Github # remotes::install_github(&quot;r-spatial/mapview&quot;) "],
["day-1-acquiring-and-cleaning-data.html", "DAY 1: Acquiring and cleaning data Acquiring data from APIs Cleaning data", " DAY 1: Acquiring and cleaning data Acquiring data from APIs Often, we want to acquire data that is stored online. Online data sources are stored somewhere on a remote server — a remotely located computer that is optimized to process requests for information. Usually, we make requests using a browser, also known as a client, but we can also make requests programmatically. An Application Programming Interface (API) is the part of a remote server that receives requests and sends responses. When we make requests programmatically, the responses an API sends are typically data-based, often in some structured format like JSON or XML. For the project we’ll pursue during DataFest, we’re going to access data stored on the Harvard Dataverse. A Dataverse is open source software for repositing research data. Once data is stored in a Dataverse, it can be accessed programmatically using the Dataverse API. We will use the R package dataverse as an interface for the Dataverse API. Here are three COVID-19 datasets from the Harvard Dataverse: US data on COVID-19 cases and deaths, daily at state-level or county-level: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/HIDLTK US data on COVID-19 cases and deaths, daily at metropolitan-level: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/5B8YM8 World data on COVID-19 cases and deaths, daily at country-level: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/L20LOT As an example of how to use the Dataverse API, we’re going to extract daily data on COVID-19 cases from the U.S. at the state-level (from dataset #1 above). These data span the period from January 21st 2020 until November 29th 2020 for each U.S. state (and the District of Columbia). If you wish, you may choose to use one of the other datasets for your project. We can use the dataverse package as an interface for the API. All we need to start is a digital object identifier (DOI) for the dataset — this is the persistentId parameter at the end of the above URLs. We can then use the get_dataset() function to retrieve the metadata for the dataset: # get the digital object identifier for the Dataverse dataset DOI &lt;- &quot;doi:10.7910/DVN/HIDLTK&quot; # retrieve the contents of the dataset covid &lt;- get_dataset(DOI) The covid object is a list of metadata that includes a data frame of all the files stored within this dataset. Let’s look at the structure of this object: # view contents glimpse(covid, max.level = 1) ## List of 16 ## $ id : int 218273 ## $ datasetId : int 3679837 ## $ datasetPersistentId: chr &quot;doi:10.7910/DVN/HIDLTK&quot; ## $ storageIdentifier : chr &quot;s3://10.7910/DVN/HIDLTK&quot; ## $ versionNumber : int 46 ## $ versionMinorNumber : int 0 ## $ versionState : chr &quot;RELEASED&quot; ## $ UNF : chr &quot;UNF:6:l+QTrceV0xEn3GGLAskwEQ==&quot; ## $ lastUpdateTime : chr &quot;2020-12-01T22:15:13Z&quot; ## $ releaseTime : chr &quot;2020-12-01T22:15:13Z&quot; ## $ createTime : chr &quot;2020-12-01T22:11:46Z&quot; ## $ license : chr &quot;CC0&quot; ## $ termsOfUse : chr &quot;CC0 Waiver&quot; ## $ fileAccessRequest : logi FALSE ## $ metadataBlocks :List of 1 ## $ files :&#39;data.frame&#39;:\t12 obs. of 22 variables: ## - attr(*, &quot;class&quot;)= chr &quot;dataverse_dataset&quot; Let’s dig further and display the available files: # view available files covid$files$filename ## [1] &quot;COUNTY_MAP.zip&quot; ## [2] &quot;INDEX.txt&quot; ## [3] &quot;METRO_MAP_2018_ESRI.zip&quot; ## [4] &quot;METRO_MAP.zip&quot; ## [5] &quot;Metropolitan_statistical_areas_for_US_counties__Sept_2018.xml&quot; ## [6] &quot;Metropolitan_statistical_areas_for_US_counties__Sept_2018.zip&quot; ## [7] &quot;README.txt&quot; ## [8] &quot;STATE_MAP.zip&quot; ## [9] &quot;us_county_confirmed_cases.tab&quot; ## [10] &quot;us_county_deaths_cases.tab&quot; ## [11] &quot;us_state_confirmed_case.tab&quot; ## [12] &quot;us_state_deaths_case.tab&quot; For our example project, we’re going to use the data on cumulative COVID-19 cases at the state-level contained in the us_state_confirmed_case.tab file. We can use the get_file() function to extract these data into a raw vector: # get data file for COVID-19 cases US_cases_file &lt;- get_file(&quot;us_state_confirmed_case.tab&quot;, dataset = DOI) To convert the data from the raw vector into a more user friendly data frame, we have to jump through a few hoops: # create a temporary file tmp &lt;- tempfile(fileext = &quot;.tab&quot;) # write the US_cases_file object to the temporary file writeBin(object = US_cases_file, con = tmp) # read the data from the temporary file into a data frame US_cases &lt;- read_csv(tmp) We can now examine the structure of the data if we uncomment the line with glimpse(US_cases) (warning — the print out is long): # inspect the data (warning --- the print out is long) # glimpse(US_cases) # 50 states plus DC by 314 days Cleaning data COVID-19 cases data The COVID-19 cases data are in wide format, with individual columns for each day’s case counts. To visualize and analyze the data, it will be much easier to reshape the data so that it is organized in long format, with a single column for case counts and another column indicating the date those counts are associated with. In addition, it will be useful to derive some time-related variables (e.g., day of year, week of year) from the dates. Finally, we should transform our cumulative case counts into regular counts and create some rate variables by normalizing by population count. US_cases_long &lt;- US_cases %&gt;% # select columns of interest select(fips, NAME, POP10, matches(&quot;^\\\\d&quot;)) %&gt;% # rename some columns rename(GEOID = fips, state = NAME, pop_count_2010 = POP10) %&gt;% # reshape to long format for dates pivot_longer(cols = grep(&quot;^\\\\d&quot;, colnames(.), value = TRUE), names_to = &quot;date&quot;, values_to = &quot;cases_cum&quot;) %&gt;% # create new derived time variables from dates mutate(date = ymd(date), # year-month-day format day_of_year = yday(date), week_of_year = week(date), month = month(date)) %&gt;% group_by(state) %&gt;% # create cases counts mutate(cases_count = cases_cum - lag(cases_cum, default = 0), # tidy-up negative counts cases_count_pos = ifelse(cases_count &lt; 0, 0, cases_count), # create cases rates cases_rate_100K = (cases_count_pos / pop_count_2010) * 1e5, cases_cum_rate_100K = (cases_cum / pop_count_2010) * 1e5) glimpse(US_cases_long) # 16014 observations (50 states + 1 DC * 314 days) ## Rows: 16,014 ## Columns: 12 ## Groups: state [51] ## $ GEOID &lt;chr&gt; &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;… ## $ state &lt;chr&gt; &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alab… ## $ pop_count_2010 &lt;dbl&gt; 4779736, 4779736, 4779736, 4779736, 4779736, 4779… ## $ date &lt;date&gt; 2020-01-21, 2020-01-22, 2020-01-23, 2020-01-24, … ## $ cases_cum &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ day_of_year &lt;dbl&gt; 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 3… ## $ week_of_year &lt;dbl&gt; 3, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 6, 6… ## $ month &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2… ## $ cases_count &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ cases_count_pos &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ cases_rate_100K &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ cases_cum_rate_100K &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… Aggregate data The cleaned data object US_cases_long has 16,014 observations (50 states + 1 DC * 314 days). For visualization, this should be fine in most cases. When we come to build models for these data, they may take a long time to run. If we’re mainly interested in longer term trends, we can probably get a good approximation by aggregating the data to the weekly level for modeling: # aggregate to weekly level (for later modeling) US_cases_long_week &lt;- US_cases_long %&gt;% group_by(GEOID, state, week_of_year) %&gt;% summarize(pop_count_2010 = mean(pop_count_2010), cases_count_pos = sum(cases_count_pos), cases_rate_100K = sum(cases_rate_100K)) %&gt;% drop_na() glimpse(US_cases_long_week) ## Rows: 2,346 ## Columns: 6 ## Groups: GEOID, state [51] ## $ GEOID &lt;chr&gt; &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;,… ## $ state &lt;chr&gt; &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;… ## $ week_of_year &lt;dbl&gt; 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, … ## $ pop_count_2010 &lt;dbl&gt; 4779736, 4779736, 4779736, 4779736, 4779736, 4779736,… ## $ cases_count_pos &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 39, 203, 757, 1198, 1756, 137… ## $ cases_rate_100K &lt;dbl&gt; 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000… "],
["optional-u-s-census-data.html", "OPTIONAL: U.S. Census data U.S. Census Bureau API Clean U.S. Census data Combine Census and COVID-19 data Aggregate to weekly-level", " OPTIONAL: U.S. Census data This section is optional. It provides an example of how to acquire potentially interesting predictors of COVID-19 cases from the U.S. Census Bureau. The COVID-19 dataset we accessed above provides daily COVID-19 case counts for each U.S State, together with population counts from the 2010 Decennial Census. This should be enough information to produce some interesting visualizations. For modeling, however, we really only have one useful predictor in the dataset — time. This section describes some options for acquiring other potentially interesting predictors of COVID-19 cases. U.S. Census Bureau API We may want to use additional demographic information in our visualizations and analysis of the COVID-19 cases. An obvious place to source this information is from the U.S. Census Bureau. There are three U.S. Census Bureau data sources, each with their own API: Decennial Census: survey of every household in the U.S. every 10 years — used to calculate population of U.S. geographic areas. American Community Survey: yearly representative sample of 3.5 million households — used to calculate population estimates of U.S. geographic areas. Population Estimates: yearly population estimates of U.S. geographic areas. The COVID-19 data from Dataverse already contains population values from the 2010 decennial census. But, using the Census Bureau’s Population Estimates API, we can get updated population data for 2019 as well as population data stratified by age groups, race, and sex. We’re going to use the tidycensus package as an interface to the Census Bureau API. A basic usage guide is available — https://walker-data.com/tidycensus/articles/basic-usage.html — but we’ll walk through all the necessary steps. The first step is to sign-up for an API key: http://api.census.gov/data/key_signup.html. Then give the key a name. # store API key API_key &lt;- &quot;your-API-key-here&quot; We can then set the API key for our current R session using the census_api_key() function (or we can include it in an .Renviron file for future use): # set API key for current session census_api_key(API_key) Next, we can use the get_estimates() function to access the Population Estimates API and extract variables of interest: pop &lt;- get_estimates( geography = &quot;state&quot;, # we&#39;ll select state-level data product = &quot;population&quot;, # provides overall population estimates and population densities year = 2019, # the latest year available key = API_key) glimpse(pop) ## Rows: 104 ## Columns: 4 ## $ NAME &lt;chr&gt; &quot;Mississippi&quot;, &quot;Missouri&quot;, &quot;Montana&quot;, &quot;Nebraska&quot;, &quot;Nevada&quot;, … ## $ GEOID &lt;chr&gt; &quot;28&quot;, &quot;29&quot;, &quot;30&quot;, &quot;31&quot;, &quot;32&quot;, &quot;33&quot;, &quot;34&quot;, &quot;35&quot;, &quot;36&quot;, &quot;37&quot;, … ## $ variable &lt;chr&gt; &quot;POP&quot;, &quot;POP&quot;, &quot;POP&quot;, &quot;POP&quot;, &quot;POP&quot;, &quot;POP&quot;, &quot;POP&quot;, &quot;POP&quot;, &quot;POP… ## $ value &lt;dbl&gt; 2976149, 6137428, 1068778, 1934408, 3080156, 1359711, 888219… Get population estimates by age group: age &lt;- get_estimates( geography = &quot;state&quot;, product = &quot;characteristics&quot;, # provides population estimates stratified by the variable specified in `breakdown` breakdown = &quot;AGEGROUP&quot;, # population estimates for different age groups breakdown_labels = TRUE, # labels for age groups year = 2019, key = API_key) glimpse(age) ## Rows: 1,664 ## Columns: 4 ## $ GEOID &lt;chr&gt; &quot;28&quot;, &quot;28&quot;, &quot;28&quot;, &quot;28&quot;, &quot;28&quot;, &quot;28&quot;, &quot;28&quot;, &quot;28&quot;, &quot;28&quot;, &quot;28&quot;, … ## $ NAME &lt;chr&gt; &quot;Mississippi&quot;, &quot;Mississippi&quot;, &quot;Mississippi&quot;, &quot;Mississippi&quot;, … ## $ value &lt;dbl&gt; 2976149, 183478, 189377, 206282, 201350, 201517, 206989, 186… ## $ AGEGROUP &lt;fct&gt; All ages, Age 0 to 4 years, Age 5 to 9 years, Age 10 to 14 y… Get population estimates by sex: sex &lt;- get_estimates( geography = &quot;state&quot;, product = &quot;characteristics&quot;, breakdown = &quot;SEX&quot;, # population estimates for different sexes breakdown_labels = TRUE, year = 2019, key = API_key) glimpse(sex) ## Rows: 156 ## Columns: 4 ## $ GEOID &lt;chr&gt; &quot;28&quot;, &quot;28&quot;, &quot;28&quot;, &quot;29&quot;, &quot;29&quot;, &quot;29&quot;, &quot;30&quot;, &quot;30&quot;, &quot;30&quot;, &quot;31&quot;, &quot;31… ## $ NAME &lt;chr&gt; &quot;Mississippi&quot;, &quot;Mississippi&quot;, &quot;Mississippi&quot;, &quot;Missouri&quot;, &quot;Misso… ## $ value &lt;dbl&gt; 2976149, 1442292, 1533857, 6137428, 3012662, 3124766, 1068778, … ## $ SEX &lt;chr&gt; &quot;Both sexes&quot;, &quot;Male&quot;, &quot;Female&quot;, &quot;Both sexes&quot;, &quot;Male&quot;, &quot;Female&quot;,… Get population estimates by race: race &lt;- get_estimates( geography = &quot;state&quot;, product = &quot;characteristics&quot;, breakdown =&quot;RACE&quot;, # population estimates for different races breakdown_labels = TRUE, year = 2019, key = API_key) glimpse(race) ## Rows: 613 ## Columns: 4 ## $ GEOID &lt;chr&gt; &quot;28&quot;, &quot;28&quot;, &quot;28&quot;, &quot;28&quot;, &quot;28&quot;, &quot;28&quot;, &quot;28&quot;, &quot;28&quot;, &quot;28&quot;, &quot;28&quot;, &quot;28… ## $ NAME &lt;chr&gt; &quot;Mississippi&quot;, &quot;Mississippi&quot;, &quot;Mississippi&quot;, &quot;Mississippi&quot;, &quot;Mi… ## $ value &lt;dbl&gt; 2976149, 1758081, 1124559, 18705, 33032, 1806, 39966, 1792535, … ## $ RACE &lt;chr&gt; &quot;All races&quot;, &quot;White alone&quot;, &quot;Black alone&quot;, &quot;American Indian and… Clean U.S. Census data The Census data we extracted contain population estimates for multiple categories of age, race, and sex. It will be useful to simplify these data by creating some derived variables that may be of interest when visualizing and analyzing the data. For example, for each state, we may want to calculate: Overall population count and density Proportion of people that are 65 years and older Proportion of people that are female (or male) Proportion of people that are black (or white, or other race) Overall population estimates: pop_wide &lt;- pop %&gt;% # order by GEOID (same as state FIPS code) arrange(GEOID) %&gt;% # rename state rename(state = NAME) %&gt;% # exclude Puerto Rico filter(state != &quot;Puerto Rico&quot;) %&gt;% # reshape population variables to wide format pivot_wider(names_from = variable, values_from = value) %&gt;% # rename population variables rename(pop_count_2019 = POP, pop_density_2019 = DENSITY) glimpse(pop_wide) ## Rows: 51 ## Columns: 4 ## $ state &lt;chr&gt; &quot;Alabama&quot;, &quot;Alaska&quot;, &quot;Arizona&quot;, &quot;Arkansas&quot;, &quot;Califor… ## $ GEOID &lt;chr&gt; &quot;01&quot;, &quot;02&quot;, &quot;04&quot;, &quot;05&quot;, &quot;06&quot;, &quot;08&quot;, &quot;09&quot;, &quot;10&quot;, &quot;11&quot;… ## $ pop_count_2019 &lt;dbl&gt; 4903185, 731545, 7278717, 3017804, 39512223, 5758736… ## $ pop_density_2019 &lt;dbl&gt; 96.811652, 1.281127, 64.043252, 57.992836, 253.52068… Population estimates by age group: age_wide &lt;- age %&gt;% # order by GEOID (same as state FIPS code) arrange(GEOID) %&gt;% # rename state rename(state = NAME) %&gt;% # reshape the age groups to wide format pivot_wider(names_from = AGEGROUP, values_from = value) %&gt;% # create variable for percentortion of people that are 65 years and older mutate(percent_age65over = (`65 years and over` / `All ages`) * 100) %&gt;% # select columns of interest select(GEOID, state, percent_age65over) glimpse(age_wide) ## Rows: 52 ## Columns: 3 ## $ GEOID &lt;chr&gt; &quot;01&quot;, &quot;02&quot;, &quot;04&quot;, &quot;05&quot;, &quot;06&quot;, &quot;08&quot;, &quot;09&quot;, &quot;10&quot;, &quot;11… ## $ state &lt;chr&gt; &quot;Alabama&quot;, &quot;Alaska&quot;, &quot;Arizona&quot;, &quot;Arkansas&quot;, &quot;Califo… ## $ percent_age65over &lt;dbl&gt; 17.33235, 12.51980, 17.97890, 17.35971, 14.77547, 1… Population estimates by sex: sex_wide &lt;- sex %&gt;% # order by GEOID (same as state FIPS code) arrange(GEOID) %&gt;% # rename state rename(state = NAME) %&gt;% # reshape the sex categories to wide format pivot_wider(names_from = SEX, values_from = value) %&gt;% # create variable for percentortion of people that are female mutate(percent_female = (Female / `Both sexes`) * 100) %&gt;% # select columns of interest select(GEOID, state, percent_female) glimpse(sex_wide) ## Rows: 52 ## Columns: 3 ## $ GEOID &lt;chr&gt; &quot;01&quot;, &quot;02&quot;, &quot;04&quot;, &quot;05&quot;, &quot;06&quot;, &quot;08&quot;, &quot;09&quot;, &quot;10&quot;, &quot;11&quot;, … ## $ state &lt;chr&gt; &quot;Alabama&quot;, &quot;Alaska&quot;, &quot;Arizona&quot;, &quot;Arkansas&quot;, &quot;Californi… ## $ percent_female &lt;dbl&gt; 51.67392, 47.86131, 50.30310, 50.90417, 50.28158, 49.6… Population estimates by race: race_wide &lt;- race %&gt;% # order by GEOID (same as state FIPS code) arrange(GEOID) %&gt;% # rename state rename(state = NAME) %&gt;% # reshape the race categories to wide format pivot_wider(names_from = RACE, values_from = value) %&gt;% # create variables for percentortion of people that are black and white mutate(percent_white = (`White alone` / `All races`) * 100, percent_black = (`Black alone` / `All races`) * 100) %&gt;% # select columns of interest select(GEOID, state, percent_white, percent_black) glimpse(race_wide) ## Rows: 52 ## Columns: 4 ## $ GEOID &lt;chr&gt; &quot;01&quot;, &quot;02&quot;, &quot;04&quot;, &quot;05&quot;, &quot;06&quot;, &quot;08&quot;, &quot;09&quot;, &quot;10&quot;, &quot;11&quot;, &quot;… ## $ state &lt;chr&gt; &quot;Alabama&quot;, &quot;Alaska&quot;, &quot;Arizona&quot;, &quot;Arkansas&quot;, &quot;California… ## $ percent_white &lt;dbl&gt; 69.12641, 65.27117, 82.61679, 79.03953, 71.93910, 86.93… ## $ percent_black &lt;dbl&gt; 26.7844473, 3.7055820, 5.1794430, 15.6752393, 6.4606767… We can now merge all the cleaned Census data into one object called demographics: demographics &lt;- list(pop_wide, age_wide, sex_wide, race_wide) %&gt;% reduce(left_join, by = c(&quot;GEOID&quot;, &quot;state&quot;)) glimpse(demographics) ## Rows: 51 ## Columns: 8 ## $ state &lt;chr&gt; &quot;Alabama&quot;, &quot;Alaska&quot;, &quot;Arizona&quot;, &quot;Arkansas&quot;, &quot;Califo… ## $ GEOID &lt;chr&gt; &quot;01&quot;, &quot;02&quot;, &quot;04&quot;, &quot;05&quot;, &quot;06&quot;, &quot;08&quot;, &quot;09&quot;, &quot;10&quot;, &quot;11… ## $ pop_count_2019 &lt;dbl&gt; 4903185, 731545, 7278717, 3017804, 39512223, 575873… ## $ pop_density_2019 &lt;dbl&gt; 96.811652, 1.281127, 64.043252, 57.992836, 253.5206… ## $ percent_age65over &lt;dbl&gt; 17.33235, 12.51980, 17.97890, 17.35971, 14.77547, 1… ## $ percent_female &lt;dbl&gt; 51.67392, 47.86131, 50.30310, 50.90417, 50.28158, 4… ## $ percent_white &lt;dbl&gt; 69.12641, 65.27117, 82.61679, 79.03953, 71.93910, 8… ## $ percent_black &lt;dbl&gt; 26.7844473, 3.7055820, 5.1794430, 15.6752393, 6.460… Combine Census and COVID-19 data Merge the COVID-19 cases data with Census demographic data: # merge COVID-19 cases with demographics US_cases_long_demogr &lt;- US_cases_long %&gt;% left_join(demographics, by = c(&quot;GEOID&quot;, &quot;state&quot;)) # update the death rate variables to use population estimates from 2019 US_cases_long_demogr &lt;- US_cases_long_demogr %&gt;% mutate(cases_cum_rate_100K = (cases_cum / pop_count_2019) * 1e5, cases_rate_100K = (cases_count_pos / pop_count_2019) * 1e5) glimpse(US_cases_long_demogr) ## Rows: 16,014 ## Columns: 18 ## Groups: state [51] ## $ GEOID &lt;chr&gt; &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;… ## $ state &lt;chr&gt; &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alab… ## $ pop_count_2010 &lt;dbl&gt; 4779736, 4779736, 4779736, 4779736, 4779736, 4779… ## $ date &lt;date&gt; 2020-01-21, 2020-01-22, 2020-01-23, 2020-01-24, … ## $ cases_cum &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ day_of_year &lt;dbl&gt; 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 3… ## $ week_of_year &lt;dbl&gt; 3, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 6, 6… ## $ month &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2… ## $ cases_count &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ cases_count_pos &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ cases_rate_100K &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ cases_cum_rate_100K &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ pop_count_2019 &lt;dbl&gt; 4903185, 4903185, 4903185, 4903185, 4903185, 4903… ## $ pop_density_2019 &lt;dbl&gt; 96.81165, 96.81165, 96.81165, 96.81165, 96.81165,… ## $ percent_age65over &lt;dbl&gt; 17.33235, 17.33235, 17.33235, 17.33235, 17.33235,… ## $ percent_female &lt;dbl&gt; 51.67392, 51.67392, 51.67392, 51.67392, 51.67392,… ## $ percent_white &lt;dbl&gt; 69.12641, 69.12641, 69.12641, 69.12641, 69.12641,… ## $ percent_black &lt;dbl&gt; 26.78445, 26.78445, 26.78445, 26.78445, 26.78445,… Aggregate to weekly-level Once again, for the purposes of modeling, it may be useful to aggregate to the weekly-level: # COVID-19 data and demographic data US_cases_long_demogr_week &lt;- US_cases_long_demogr %&gt;% group_by(state, week_of_year) %&gt;% summarize(pop_count_2019 = mean(pop_count_2019), percent_age65over = mean(percent_age65over), percent_female = mean(percent_female), percent_white = mean(percent_white), percent_black = mean(percent_black), cases_count_pos = sum(cases_count_pos), cases_rate_100K = sum(cases_rate_100K)) %&gt;% drop_na() Let’s store the data frame in a binary R file so that we can easily access it later: save(US_cases_long_demogr_week, file = &quot;US_cases_long_demogr_week.Rdata&quot;) "],
["day-2-data-visualization.html", "DAY 2: Data visualization Non-spatial graphs Static Maps Interactive Maps", " DAY 2: Data visualization The COVID-19 cases data we have are inherently temporal and spatial. Let’s explore the space and time dimensions of the case data through visualization. Non-spatial graphs We can easily create a wide range of non-spatial (and spatial) graphs using the ggplot() function from the ggplot2 package. If you need a refresher on this package, both IQSS and HBS collaborate on delivering a workshop devoted to ggplot2 each semester and the workshop materials can be accessed here: https://iqss.github.io/dss-workshops/Rgraphics.html. We can start with a very simple line graph of the COVID-19 cases rates over time: # line graph of covid cases rates ggplot(US_cases_long, aes(x = date, y = cases_rate_100K)) + geom_line() + theme_classic() This gives us an overall sense that the rate of cases has increased over time and has become particularly prevalent in the fall of 2020. But, because the lines for each state are not discernible, we can’t see if some states have a different trajectory of case rates than other states. We could try making each state’s line a different color, but with 50 states plus D.C., we won’t be able to easily identify which color hue is associated with which state. A better solution is to use faceting to produce mini-plots for each state. Let’s create a new line graph of COVID-19 cases rates over time, this time with a separate mini-plot for each state: # line graphs of covid cases rates for each state ggplot(US_cases_long, aes(x = date, y = cases_rate_100K)) + geom_line() + facet_wrap(~ state, scales = &quot;free_y&quot;) + # make the y-axis independent for each state theme_classic() We can try the same strategy for cumulative COVID-19 case rates over time. First, in a graph that jumbles together all the states: # line graph of cumulative covid cases rates ggplot(US_cases_long, aes(x = date, y = cases_cum_rate_100K)) + geom_line() + theme_classic() Again, we get a sense of the overall trend here, but we can get a much better picture of state-level differences by faceting. So, let’s create a new line graph of COVID-19 cumulative cases rates over time, this time with a separate mini-plot for each state: # line graphs of cumulative covid cases rates for each state ggplot(US_cases_long, aes(x = date, y = cases_cum_rate_100K)) + geom_line() + facet_wrap(~ state, scales = &quot;free_y&quot;) + theme_classic() Static Maps A great way to visualize spatial relationships in data is to superimpose variables onto a map. For some datasets, this could involve superimposing points or lines. For our state-level data, this will involve coloring state polygons in proportion to a variable of interest that represents an aggregate summary of a geographic characteristic within each state. Such a graph is often referred to as a choropleth map. To create a choropleth map we first need to acquire shapefiles that contain spatial data about U.S. state-level geographies. We can use the tigris package to get Census Tiger shapefiles for census geographies. In particular, we can use the states() function to get state-level geographies, and coastal boundaries can be gathered with the argument cb = TRUE: # download state-level census geographies us_state_geo &lt;- tigris::states(class = &quot;sf&quot;, cb = TRUE) %&gt;% # rename `NAME` variable to `state` rename(state = NAME) glimpse(us_state_geo) ## Rows: 56 ## Columns: 10 ## $ STATEFP &lt;chr&gt; &quot;12&quot;, &quot;78&quot;, &quot;30&quot;, &quot;27&quot;, &quot;24&quot;, &quot;45&quot;, &quot;23&quot;, &quot;15&quot;, &quot;11&quot;, &quot;69&quot;, … ## $ STATENS &lt;chr&gt; &quot;00294478&quot;, &quot;01802710&quot;, &quot;00767982&quot;, &quot;00662849&quot;, &quot;01714934&quot;, … ## $ AFFGEOID &lt;chr&gt; &quot;0400000US12&quot;, &quot;0400000US78&quot;, &quot;0400000US30&quot;, &quot;0400000US27&quot;, … ## $ GEOID &lt;chr&gt; &quot;12&quot;, &quot;78&quot;, &quot;30&quot;, &quot;27&quot;, &quot;24&quot;, &quot;45&quot;, &quot;23&quot;, &quot;15&quot;, &quot;11&quot;, &quot;69&quot;, … ## $ STUSPS &lt;chr&gt; &quot;FL&quot;, &quot;VI&quot;, &quot;MT&quot;, &quot;MN&quot;, &quot;MD&quot;, &quot;SC&quot;, &quot;ME&quot;, &quot;HI&quot;, &quot;DC&quot;, &quot;MP&quot;, … ## $ state &lt;chr&gt; &quot;Florida&quot;, &quot;United States Virgin Islands&quot;, &quot;Montana&quot;, &quot;Minne… ## $ LSAD &lt;chr&gt; &quot;00&quot;, &quot;00&quot;, &quot;00&quot;, &quot;00&quot;, &quot;00&quot;, &quot;00&quot;, &quot;00&quot;, &quot;00&quot;, &quot;00&quot;, &quot;00&quot;, … ## $ ALAND &lt;dbl&gt; 138947364717, 348021896, 376966832749, 206230065476, 2515172… ## $ AWATER &lt;dbl&gt; 31362872853, 1550236199, 3869031338, 18942261495, 6979340970… ## $ geometry &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((-80.17628 2..., MULTIPOLYGON ((… We can now merge the spatial data with our weekly COVID-19 cases data, keeping only the contiguous 48 states (plus D.C.): # merge weekly COVID-19 cases with spatial data US_cases_long_week_spatial &lt;- us_state_geo %&gt;% left_join(US_cases_long_week, by = c(&quot;GEOID&quot;, &quot;state&quot;)) %&gt;% filter(GEOID &lt; 60 &amp; state != &quot;Alaska&quot; &amp; state != &quot;Hawaii&quot;) glimpse(US_cases_long_week_spatial) ## Rows: 2,254 ## Columns: 14 ## $ STATEFP &lt;chr&gt; &quot;12&quot;, &quot;12&quot;, &quot;12&quot;, &quot;12&quot;, &quot;12&quot;, &quot;12&quot;, &quot;12&quot;, &quot;12&quot;, &quot;12&quot;,… ## $ STATENS &lt;chr&gt; &quot;00294478&quot;, &quot;00294478&quot;, &quot;00294478&quot;, &quot;00294478&quot;, &quot;0029… ## $ AFFGEOID &lt;chr&gt; &quot;0400000US12&quot;, &quot;0400000US12&quot;, &quot;0400000US12&quot;, &quot;0400000… ## $ GEOID &lt;chr&gt; &quot;12&quot;, &quot;12&quot;, &quot;12&quot;, &quot;12&quot;, &quot;12&quot;, &quot;12&quot;, &quot;12&quot;, &quot;12&quot;, &quot;12&quot;,… ## $ STUSPS &lt;chr&gt; &quot;FL&quot;, &quot;FL&quot;, &quot;FL&quot;, &quot;FL&quot;, &quot;FL&quot;, &quot;FL&quot;, &quot;FL&quot;, &quot;FL&quot;, &quot;FL&quot;,… ## $ state &lt;chr&gt; &quot;Florida&quot;, &quot;Florida&quot;, &quot;Florida&quot;, &quot;Florida&quot;, &quot;Florida&quot;… ## $ LSAD &lt;chr&gt; &quot;00&quot;, &quot;00&quot;, &quot;00&quot;, &quot;00&quot;, &quot;00&quot;, &quot;00&quot;, &quot;00&quot;, &quot;00&quot;, &quot;00&quot;,… ## $ ALAND &lt;dbl&gt; 138947364717, 138947364717, 138947364717, 13894736471… ## $ AWATER &lt;dbl&gt; 31362872853, 31362872853, 31362872853, 31362872853, 3… ## $ week_of_year &lt;dbl&gt; 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, … ## $ pop_count_2010 &lt;dbl&gt; 18801310, 18801310, 18801310, 18801310, 18801310, 188… ## $ cases_count_pos &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 3, 20, 187, 1257, 5275, 7997, 6881,… ## $ cases_rate_100K &lt;dbl&gt; 0.00000000, 0.00000000, 0.00000000, 0.00000000, 0.000… ## $ geometry &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((-80.17628 2..., MULTIPOL… Let’s create a choropleth map for the latest week’s COVID-19 cases using ggplot(): US_cases_long_week_spatial %&gt;% # subset data for only latest week filter(week_of_year == max(week_of_year, na.rm = TRUE)) %&gt;% # map starts here ggplot(aes(fill = cases_rate_100K, color = cases_rate_100K)) + geom_sf() + coord_sf(crs = 5070, datum = NA) + scale_fill_viridis(direction = -1) + scale_color_viridis(direction = -1) + labs(title = &quot;COVID-19 cases for last week&quot;, caption = &quot;Data Source: U.S. Census Bureau; 2019&quot;) Interactive Maps Static maps are great for publications. Interactive maps, which can be viewed in a browser, can potentially provide a much richer source of information. A good overview of the mapping functionality in R is provided here: https://map-rfun.library.duke.edu/index.html. In this section, we’ll focus on building a simple interactive map using the mapview package, which is a data-driven API for the leaflet package. # set some options for the graph mapviewOptions(fgb = FALSE, # set to FALSE to embed data directly into the HTML leafletWidth = 800, legend.pos = &quot;bottomright&quot;) # create map USmap &lt;- US_cases_long_week_spatial %&gt;% # subset data for only latest week filter(week_of_year == max(week_of_year, na.rm = TRUE)) %&gt;% # map starts here mapview(zcol = &quot;cases_rate_100K&quot;, layer.name = &quot;Cases (rate per 100K)&quot;) # print map USmap@map "],
["day-3-data-analysis.html", "DAY 3: Data analysis Descriptives Modeling", " DAY 3: Data analysis In this section, we will be exploring the relationships between COVID-19 cases and demographic data from the Census Bureau. If you did not complete the optional Census data section, you can still access these data by loading the following file: load(&quot;US_cases_long_demogr_week.Rdata&quot;) glimpse(US_cases_long_demogr_week) ## Rows: 2,346 ## Columns: 9 ## Groups: state [51] ## $ state &lt;chr&gt; &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabam… ## $ week_of_year &lt;dbl&gt; 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17… ## $ pop_count_2019 &lt;dbl&gt; 4903185, 4903185, 4903185, 4903185, 4903185, 490318… ## $ percent_age65over &lt;dbl&gt; 17.33235, 17.33235, 17.33235, 17.33235, 17.33235, 1… ## $ percent_female &lt;dbl&gt; 51.67392, 51.67392, 51.67392, 51.67392, 51.67392, 5… ## $ percent_white &lt;dbl&gt; 69.12641, 69.12641, 69.12641, 69.12641, 69.12641, 6… ## $ percent_black &lt;dbl&gt; 26.78445, 26.78445, 26.78445, 26.78445, 26.78445, 2… ## $ cases_count_pos &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 39, 203, 757, 1198, 1756, 1… ## $ cases_rate_100K &lt;dbl&gt; 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.00000… Descriptives It’s always a good idea to start data analysis by looking at some descriptive statistics of the sample data. Here, we can inspect the demographic data we accessed through the Census API: US_cases_long_demogr_week %&gt;% group_by(state) %&gt;% summarize_at(vars(percent_age65over, percent_female, percent_white, percent_black), .funs = mean) %&gt;% mutate_if(is.numeric, round, 1) ## # A tibble: 51 x 5 ## state percent_age65ov… percent_female percent_white percent_black ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Alabama 17.3 51.7 69.1 26.8 ## 2 Alaska 12.5 47.9 65.3 3.7 ## 3 Arizona 18 50.3 82.6 5.2 ## 4 Arkansas 17.4 50.9 79 15.7 ## 5 California 14.8 50.3 71.9 6.5 ## 6 Colorado 14.6 49.6 86.9 4.6 ## 7 Connecticut 17.7 51.2 79.7 12.2 ## 8 Delaware 19.4 51.7 69.2 23.2 ## 9 District of Colu… 12.4 52.6 46 46 ## 10 Florida 20.9 51.1 77.3 16.9 ## # … with 41 more rows Modeling The data we have consists of counts of COVID-19 cases over time for each of 50 U.S. states and D.C. These data will be challenging to model, since we will have to deal with the following issues: The response consists of counts with a huge number of zeros and an extended right tail. Typically, to model counts we’d use a poisson model. Here, the extended right tail suggests the data are overdispersed (i.e., the variance is greater than the mean), which would mean the restrictive assumptions of the poisson distribution are not met and may push us towards a quasi-poisson or negative binomial model. In addition, we may need some machinery in the model to deal with the excess of zeros (a zero-inflation component), since this is atypical for a poisson or negative binomial model. Let’s inspect the response variable: # response summary summary(US_cases_long_demogr_week$cases_count_pos) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0 175.5 2001.0 5695.7 6093.2 94909.0 # marginal response distribution (truncated to counts &lt; 1000) ggplot(US_cases_long_demogr_week %&gt;% filter(cases_count_pos &lt; 1000), aes(x = cases_count_pos)) + geom_histogram(bins = 1e3) + theme_classic() The data are inherently spatial in nature — in this case, at the state-level. The data are inherently temporal in nature — in this case, at the daily- or weekly-level. Cross-sectional models Let’s start with something at the simpler end of the scale. We can reduce complexity by initially modeling a single time point (for example, the most recent week of case data), with a subset of states, and just a single predictor — the intercept — to estimate the average number of cases. # filter the most recent week&#39;s data US_cases_latest_week &lt;- US_cases_long_demogr_week %&gt;% filter(week_of_year == max(week_of_year)) glimpse(US_cases_latest_week) ## Rows: 51 ## Columns: 9 ## Groups: state [51] ## $ state &lt;chr&gt; &quot;Alabama&quot;, &quot;Alaska&quot;, &quot;Arizona&quot;, &quot;Arkansas&quot;, &quot;Califo… ## $ week_of_year &lt;dbl&gt; 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48,… ## $ pop_count_2019 &lt;dbl&gt; 4903185, 731545, 7278717, 3017804, 39512223, 575873… ## $ percent_age65over &lt;dbl&gt; 17.33235, 12.51980, 17.97890, 17.35971, 14.77547, 1… ## $ percent_female &lt;dbl&gt; 51.67392, 47.86131, 50.30310, 50.90417, 50.28158, 4… ## $ percent_white &lt;dbl&gt; 69.12641, 65.27117, 82.61679, 79.03953, 71.93910, 8… ## $ percent_black &lt;dbl&gt; 26.7844473, 3.7055820, 5.1794430, 15.6752393, 6.460… ## $ cases_count_pos &lt;dbl&gt; 10364, 3188, 18052, 7935, 66491, 22175, 5301, 2587,… ## $ cases_rate_100K &lt;dbl&gt; 211.37281, 435.79001, 248.01074, 262.93954, 168.279… Now let’s inspect the response variable for just this last week of data: # histogram of last week&#39;s counts ggplot(US_cases_latest_week, aes(x = cases_count_pos)) + geom_histogram(bins = 50) + theme_classic() # number of cases in sample summary(US_cases_latest_week$cases_count_pos) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 338 5106 10364 15163 20009 66491 Usually with count data, we’d fit a model designed to deal with the idiosyncrasies of counts — which are integer-only, lower bounded at zero, and generally heavily right skewed — such as a poisson, quasi-poisson, or negative binomial model. Here, however, the average number of counts is high and we don’t have any observations near the theoretical lower boundary of zero, so we can try a basic linear model since in this situation the Gaussian family of distributions approximates the poisson. # fit intercept-only OLS model model_last_week1 &lt;- lm(cases_count_pos ~ 1, data = US_cases_latest_week) # inference summary(model_last_week1) ## ## Call: ## lm(formula = cases_count_pos ~ 1, data = US_cases_latest_week) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14825 -10058 -4799 4846 51328 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 15163 1981 7.655 0.00000000057 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 14150 on 50 degrees of freedom confint(model_last_week1) ## 2.5 % 97.5 % ## (Intercept) 11184.68 19141.6 # model diagnostics - simulate residuals using the DHARMa package model_last_week1_simres &lt;- simulateResiduals(model_last_week1) plot(model_last_week1_simres, quantreg = TRUE) We recovered the average number of cases for the latest week, pooled over all the states. Now we can try adding some of our explanatory variables. # fit OLS model model_last_week2 &lt;- lm(cases_count_pos ~ 1 + percent_age65over + percent_female + percent_black, data = US_cases_latest_week) # inference summary(model_last_week2) ## ## Call: ## lm(formula = cases_count_pos ~ 1 + percent_age65over + percent_female + ## percent_black, data = US_cases_latest_week) ## ## Residuals: ## Min 1Q Median 3Q Max ## -23275 -7096 -4382 3385 46178 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -312043.4 204588.9 -1.525 0.1339 ## percent_age65over -2358.0 1242.6 -1.898 0.0639 . ## percent_female 7363.1 4334.8 1.699 0.0960 . ## percent_black -469.0 341.1 -1.375 0.1756 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 13970 on 47 degrees of freedom ## Multiple R-squared: 0.08298,\tAdjusted R-squared: 0.02445 ## F-statistic: 1.418 on 3 and 47 DF, p-value: 0.2494 # model diagnostics - simulate residuals using the DHARMa package model_last_week2_simres &lt;- simulateResiduals(model_last_week2) plot(model_last_week2_simres, quantreg = TRUE) We’re not able to detect any effects of interest here — perhaps because we’re only using one week of data. We actually have a year’s worth of data, so let’s try modeling this as a panel (a longitudinal dataset). Panel models We have case count data for each state, tracked at the weekly-level for a year. This means that the data are clustered at the state-level (i.e., observations within states are likely to be correlated with one another more than observations between different states). We could deal with this clustering in several different ways, but using a multi-level model with random intercepts grouped by state is a good, flexible option. Let’s start with a linear model. model_panel1 &lt;- glmmTMB(cases_count_pos ~ 1 + week_of_year + percent_age65over + percent_female + percent_black + (1 | state), family = gaussian(link = &quot;identity&quot;), data = US_cases_long_demogr_week) # inference summary(model_panel1) ## Family: gaussian ( identity ) ## Formula: ## cases_count_pos ~ 1 + week_of_year + percent_age65over + percent_female + ## percent_black + (1 | state) ## Data: US_cases_long_demogr_week ## ## AIC BIC logLik deviance df.resid ## NA NA NA NA 2339 ## ## Random effects: ## ## Conditional model: ## Groups Name Variance Std.Dev. ## state (Intercept) 0.002297 0.04793 ## Residual 91426064.955941 9561.69781 ## Number of obs: 2346, groups: state, 51 ## ## Dispersion estimate for gaussian family (sigma^2): 9.14e+07 ## ## Conditional model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 2.023 20906.193 0.000 1.000 ## week_of_year 327.253 14.870 22.008 &lt; 0.0000000000000002 *** ## percent_age65over -597.360 125.788 -4.749 0.00000204 *** ## percent_female 135.539 442.844 0.306 0.760 ## percent_black 49.035 34.700 1.413 0.158 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # model diagnostics - simulate residuals using the DHARMa package model_panel1_simres &lt;- simulateResiduals(model_panel1) par(mfrow = c(1, 1)) plot(model_panel1_simres, quantreg = TRUE) Aside from the convergence warning, the model diagnostics look terrible here — why do you think that is? Now that we have a full year’s worth of data, for many states the earlier part of that year consisted of a very small number of cases — often zero cases. summary(US_cases_long_demogr_week$cases_count_pos) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0 175.5 2001.0 5695.7 6093.2 94909.0 table(US_cases_long_demogr_week$cases_count_pos == 0) ## ## FALSE TRUE ## 2006 340 About 15% of the data are zeros. This makes the linear model a poor fit for these data. Let’s try a model designed specifically for count data: model_panel2 &lt;- glmmTMB(cases_count_pos ~ 1 + week_of_year + percent_age65over + percent_female + percent_black + (1 | state), family = poisson(link = &quot;log&quot;), data = US_cases_long_demogr_week) # inference summary(model_panel2) ## Family: poisson ( log ) ## Formula: ## cases_count_pos ~ 1 + week_of_year + percent_age65over + percent_female + ## percent_black + (1 | state) ## Data: US_cases_long_demogr_week ## ## AIC BIC logLik deviance df.resid ## 6970899 6970934 -3485444 6970887 2340 ## ## Random effects: ## ## Conditional model: ## Groups Name Variance Std.Dev. ## state (Intercept) 1.189 1.091 ## Number of obs: 2346, groups: state, 51 ## ## Conditional model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -13.37143428 15.97031012 -0.8 0.4024 ## week_of_year 0.06607058 0.00002526 2615.3 &lt;0.0000000000000002 *** ## percent_age65over -0.22649142 0.09699394 -2.3 0.0195 * ## percent_female 0.46168234 0.33838023 1.4 0.1724 ## percent_black -0.00887897 0.02662506 -0.3 0.7388 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # model diagnostics - simulate residuals using the DHARMa package model_panel2_simres &lt;- simulateResiduals(model_panel2) plot(model_panel2_simres, quantreg = TRUE) This looks better. But there are several issues we’re not yet dealing with. Chief among these are: 1) the fact that states have different population levels, but our model is unaware of this, and 2) we have a huge number of zeros in the data, which poisson models are ill-equipped to handle. Let’s fit a more sophisticated model that can account for these issues. We can include an exposure term using the offset() function to get counts per population unit, as well as a separate binomial model to account for the excess of zeros (the zero-inflation component): model_panel3 &lt;- glmmTMB(cases_count_pos ~ 1 + offset(log(pop_count_2019)) + week_of_year + percent_age65over + percent_female + percent_black + (1 | state), family = poisson(link = &quot;log&quot;), ziformula = ~ week_of_year, data = US_cases_long_demogr_week) # inference summary(model_panel3) ## Family: poisson ( log ) ## Formula: ## cases_count_pos ~ 1 + offset(log(pop_count_2019)) + week_of_year + ## percent_age65over + percent_female + percent_black + (1 | state) ## Zero inflation: ~week_of_year ## Data: US_cases_long_demogr_week ## ## AIC BIC logLik deviance df.resid ## 6243994 6244040 -3121989 6243978 2338 ## ## Random effects: ## ## Conditional model: ## Groups Name Variance Std.Dev. ## state (Intercept) 0.2104 0.4587 ## Number of obs: 2346, groups: state, 51 ## ## Conditional model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 7.25747164 6.71704524 1.1 0.2799 ## week_of_year 0.05878122 0.00002682 2191.4 &lt;0.0000000000000002 *** ## percent_age65over -0.04876803 0.04079522 -1.2 0.2319 ## percent_female -0.30602496 0.14232092 -2.2 0.0315 * ## percent_black 0.01955739 0.01119827 1.7 0.0807 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Zero-inflation model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 10.51867 0.88767 11.85 &lt;0.0000000000000002 *** ## week_of_year -1.14758 0.09542 -12.03 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # model diagnostics - simulate residuals using the DHARMa package model_panel3_simres &lt;- simulateResiduals(model_panel3) plot(model_panel3_simres, quantreg = TRUE) This is looking better. Let’s compare our last two models: # use likelihood ratio test to compare models with and without # population exposure and zero-inflation component anova(model_panel2, model_panel3) ## Data: US_cases_long_demogr_week ## Models: ## model_panel2: cases_count_pos ~ 1 + week_of_year + percent_age65over + percent_female + , zi=~0, disp=~1 ## model_panel2: percent_black + (1 | state), zi=~week_of_year, disp=~1 ## model_panel3: cases_count_pos ~ 1 + offset(log(pop_count_2019)) + week_of_year + , zi=~0, disp=~1 ## model_panel3: percent_age65over + percent_female + percent_black + (1 | , zi=~week_of_year, disp=~1 ## model_panel3: state), zi=~0, disp=~1 ## Df AIC BIC logLik deviance Chisq Chi Df ## model_panel2 6 6970899 6970934 -3485444 6970887 ## model_panel3 8 6243994 6244040 -3121989 6243978 726909 2 ## Pr(&gt;Chisq) ## model_panel2 ## model_panel3 &lt; 0.00000000000000022 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Our model_panel3 does a much better job of capturing the idiosyncrasies of our data. We can go further, however. The data may not meet the restrictive assumptions of the poisson model (that the variance is equal to the mean), in which case one option is to fit a negative binomial model that can account for this over- or under-dispersion. We can also include a more flexible random effects structure, to allow each state to have different case count trajectories over time: model_panel4 &lt;- glmmTMB(cases_count_pos ~ 1 + offset(log(pop_count_2019)) + week_of_year + percent_age65over + percent_female + percent_black + (1 + week_of_year | state), family = nbinom2(link = &quot;log&quot;), ziformula = ~ week_of_year, data = US_cases_long_demogr_week) # inference summary(model_panel4) ## Family: nbinom2 ( log ) ## Formula: ## cases_count_pos ~ 1 + offset(log(pop_count_2019)) + week_of_year + ## percent_age65over + percent_female + percent_black + (1 + ## week_of_year | state) ## Zero inflation: ~week_of_year ## Data: US_cases_long_demogr_week ## ## AIC BIC logLik deviance df.resid ## 36194.6 36257.9 -18086.3 36172.6 2335 ## ## Random effects: ## ## Conditional model: ## Groups Name Variance Std.Dev. Corr ## state (Intercept) 1.043021 1.02128 ## week_of_year 0.001259 0.03549 -0.93 ## Number of obs: 2346, groups: state, 51 ## ## Overdispersion parameter for nbinom2 family (): 1.4 ## ## Conditional model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -12.450500 7.002154 -1.778 0.0754 . ## week_of_year 0.076687 0.005287 14.505 &lt;0.0000000000000002 *** ## percent_age65over -0.073991 0.035469 -2.086 0.0370 * ## percent_female 0.079872 0.146119 0.547 0.5846 ## percent_black 0.014524 0.009735 1.492 0.1357 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Zero-inflation model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 10.51630 0.88862 11.83 &lt;0.0000000000000002 *** ## week_of_year -1.14789 0.09557 -12.01 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # model diagnostics - simulate residuals using the DHARMa package model_panel4_simres &lt;- simulateResiduals(model_panel4) plot(model_panel4_simres, quantreg = TRUE) This looks like our best model yet. We can’t compare our previous poisson model and this negative binomial model directly, but we can update our model_panel3 model to use the negative binomial distribution, so that we can determine whether the random slope for time improve the model fit: # update poisson model to use negative binomial family model_panel3 &lt;- update(model_panel3, family = nbinom2) # use likelihood ratio test to compare model with and without random slopes for time anova(model_panel3, model_panel4) ## Data: US_cases_long_demogr_week ## Models: ## model_panel3: cases_count_pos ~ 1 + offset(log(pop_count_2019)) + week_of_year + , zi=~week_of_year, disp=~1 ## model_panel3: percent_age65over + percent_female + percent_black + (1 | , zi=~week_of_year, disp=~1 ## model_panel3: state), zi=~week_of_year, disp=~1 ## model_panel4: cases_count_pos ~ 1 + offset(log(pop_count_2019)) + week_of_year + , zi=~week_of_year, disp=~1 ## model_panel4: percent_age65over + percent_female + percent_black + (1 + , zi=~week_of_year, disp=~1 ## model_panel4: week_of_year | state), zi=~week_of_year, disp=~1 ## Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) ## model_panel3 9 36484 36536 -18233 36466 ## model_panel4 11 36195 36258 -18086 36173 293.69 2 &lt; 0.00000000000000022 ## ## model_panel3 ## model_panel4 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The random slope is a useful addition. Let’s look at some visualizations of the effects from the model: # plot marginal effects for all explanatory variables eff_panel4 &lt;- allEffects(model_panel4) plot(eff_panel4, type = &quot;response&quot;) So far, we’ve only been modeling a linear trend for time. From our visualizations we know that this is unrealistic. How could we incorporate non-linear time elements in the model (e.g., splines, polynomials)? "],
["day-4-data-archiving.html", "DAY 4: Data archiving", " DAY 4: Data archiving The data archiving session will probably be a GUI-based overview of using Harvard Dataverse. Though, we could put some demo code out there for how to create and populate a Dataverse entry using the API. "]
]
