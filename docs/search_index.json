[
["index.html", "DataFest2021 Project Example What is this? Project summary Authors", " DataFest2021 Project Example December 2020 What is this? The materials in this repository serve as a presenter-facing example of one possible project workflow for DataFest2021. Presenters should feel free to use, modify, extend, or otherwise wrangle this material into a form that suits their preferences. Or, they should feel free to ignore the material entirely if there is a particular direction they have in mind for their session. Untimately, each presenter should create their own R or Python script that they will work through during their session (source code, Rmarkdown, or notebook formats are all fine). The materials should not be shared with participants ahead of time, but we will upload materials to the main IQSS/datafest Github repo at the end of each day, so that participants attending only a subset of days can orientate themselves to what has been presented previously. Project summary The ‘research project’ focuses on investigating COVID-19 case rates over time and space. The goal is for participants to learn about best practices for handling and using data throughout the data life-cycle, which includes: data acquisition and cleaning, data visualization and analysis, as well as data archiving and dissemination. They will learn how to programmatically extract COVID-19 data from online sources using APIs, wrangle those data into a clean state, visualize the data temporally and spatially, analyze the data using a variety of statistical models, and finally archive the project replication files (code and data) into an online data repository (Harvard Dataverse). Authors The content in this repo was created by Steven Worthington (IQSS) and Jinjie Liu (IQSS). "],
["project-overview.html", "Project overview", " Project overview Over the next few days, we’ll be engaged in a research project investigating COVID-19 case rates over time and space. The goal is to learn about best practices for handling and using data throughout the data life-cycle, which includes: data acquisition and cleaning, data visualization and analysis, as well as data archiving and dissemination. We’ll learn how to programmatically extract COVID-19 data from online sources, wrangle those data into a clean state, visualize the data temporally and spatially, analyze the data using a variety of statistical models, and finally archive the project replication files (code and data) into an online data repository. "],
["r-setup.html", "R setup", " R setup In early January, we’ll create an installation guide for R and RStudio (based on this one: https://iqss.github.io/dss-workshops/Rinstall.html), together with instructions for installing packages, so that all participants can arrive with working R environments. For now, we just need to install and load the necessary packages for the current material: ipak &lt;- function(pkg){ new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, &quot;Package&quot;])] if(length(new.pkg)) install.packages(new.pkg, dependencies = TRUE) sapply(pkg, require, character.only = TRUE) } packages &lt;- c(&quot;viridis&quot;, &quot;glmmTMB&quot;, &quot;effects&quot;, &quot;dataverse&quot;, &quot;sf&quot;, &quot;remotes&quot;, &quot;leaflet&quot;, &quot;mapview&quot;, &quot;htmltools&quot;, &quot;htmlwidgets&quot;, &quot;tigris&quot;, &quot;lubridate&quot;, &quot;DHARMa&quot;, &quot;tidycensus&quot;, &quot;tidyverse&quot;, &quot;tidymodels&quot;) ipak(packages) ## viridis glmmTMB effects dataverse sf remotes ## TRUE TRUE TRUE TRUE TRUE TRUE ## leaflet mapview htmltools htmlwidgets tigris lubridate ## TRUE TRUE TRUE TRUE TRUE TRUE ## DHARMa tidycensus tidyverse tidymodels ## TRUE TRUE TRUE TRUE # mapview may need to be installed from Github # remotes::install_github(&quot;r-spatial/mapview&quot;) "],
["day-1-acquiring-and-cleaning-data-r.html", "DAY 1: Acquiring and cleaning data (R) Acquiring data from APIs Cleaning data", " DAY 1: Acquiring and cleaning data (R) Acquiring data from APIs Often, we want to acquire data that is stored online. Online data sources are stored somewhere on a remote server — a remotely located computer that is optimized to process requests for information. Usually, we make requests using a browser, also known as a client, but we can also make requests programmatically. An Application Programming Interface (API) is the part of a remote server that receives requests and sends responses. When we make requests programmatically, the responses an API sends are typically data-based, often in some structured format like JSON or XML. For the project we’ll pursue during DataFest, we’re going to access data stored on the Harvard Dataverse. A Dataverse is open source software for repositing research data. Once data is stored in a Dataverse, it can be accessed programmatically using the Dataverse API. We will use the R package dataverse as an interface for the Dataverse API. Here are three COVID-19 datasets from the Harvard Dataverse: US data on COVID-19 cases and deaths, daily at state-level or county-level: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/HIDLTK US data on COVID-19 cases and deaths, daily at metropolitan-level: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/5B8YM8 World data on COVID-19 cases and deaths, daily at country-level: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/L20LOT As an example of how to use the Dataverse API, we’re going to extract daily data on COVID-19 cases from the U.S. at the state-level (from dataset #1 above). These data span the period from January 21st 2020 until November 29th 2020 for each U.S. state (and the District of Columbia). If you wish, you may choose to use one of the other datasets for your project. We can use the dataverse package as an interface for the API. All we need to start is a digital object identifier (DOI) for the dataset — this is the persistentId parameter at the end of the above URLs. We can then use the get_dataset() function to retrieve the metadata for the dataset: # get the digital object identifier for the Dataverse dataset DOI &lt;- &quot;doi:10.7910/DVN/HIDLTK&quot; # retrieve the contents of the dataset covid &lt;- get_dataset(DOI) The covid object is a list of metadata that includes a data frame of all the files stored within this dataset. Let’s look at the structure of this object: # view contents glimpse(covid, max.level = 1) ## List of 16 ## $ id : int 218273 ## $ datasetId : int 3679837 ## $ datasetPersistentId: chr &quot;doi:10.7910/DVN/HIDLTK&quot; ## $ storageIdentifier : chr &quot;s3://10.7910/DVN/HIDLTK&quot; ## $ versionNumber : int 46 ## $ versionMinorNumber : int 0 ## $ versionState : chr &quot;RELEASED&quot; ## $ UNF : chr &quot;UNF:6:l+QTrceV0xEn3GGLAskwEQ==&quot; ## $ lastUpdateTime : chr &quot;2020-12-01T22:15:13Z&quot; ## $ releaseTime : chr &quot;2020-12-01T22:15:13Z&quot; ## $ createTime : chr &quot;2020-12-01T22:11:46Z&quot; ## $ license : chr &quot;CC0&quot; ## $ termsOfUse : chr &quot;CC0 Waiver&quot; ## $ fileAccessRequest : logi FALSE ## $ metadataBlocks :List of 1 ## $ files :&#39;data.frame&#39;:\t12 obs. of 22 variables: ## - attr(*, &quot;class&quot;)= chr &quot;dataverse_dataset&quot; Let’s dig further and display the available files: # view available files covid$files$filename ## [1] &quot;COUNTY_MAP.zip&quot; ## [2] &quot;INDEX.txt&quot; ## [3] &quot;METRO_MAP_2018_ESRI.zip&quot; ## [4] &quot;METRO_MAP.zip&quot; ## [5] &quot;Metropolitan_statistical_areas_for_US_counties__Sept_2018.xml&quot; ## [6] &quot;Metropolitan_statistical_areas_for_US_counties__Sept_2018.zip&quot; ## [7] &quot;README.txt&quot; ## [8] &quot;STATE_MAP.zip&quot; ## [9] &quot;us_county_confirmed_cases.tab&quot; ## [10] &quot;us_county_deaths_cases.tab&quot; ## [11] &quot;us_state_confirmed_case.tab&quot; ## [12] &quot;us_state_deaths_case.tab&quot; For our example project, we’re going to use the data on cumulative COVID-19 cases at the state-level contained in the us_state_confirmed_case.tab file. We can use the get_file() function to extract these data into a raw vector: # get data file for COVID-19 cases US_cases_file &lt;- get_file(&quot;us_state_confirmed_case.tab&quot;, dataset = DOI) To convert the data from the raw vector into a more user friendly data frame, we have to jump through a few hoops: # create a temporary file tmp &lt;- tempfile(fileext = &quot;.tab&quot;) # write the US_cases_file object to the temporary file writeBin(object = US_cases_file, con = tmp) # read the data from the temporary file into a data frame US_cases &lt;- read_csv(tmp) We can now examine the structure of the data: # inspect the data head(US_cases) # 50 states plus DC by 314 days ## # A tibble: 6 x 326 ## fips NAME POP70 HHD70 POP80 HHD80 POP90 HHD90 POP00 HHD00 POP10 ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 01 Alab… 3.43e6 1.03e6 3.89e6 1.34e6 4.04e6 1.51e6 4.45e6 1.74e6 4.78e6 ## 2 02 Alas… 2.25e5 6.15e4 3.93e5 1.30e5 5.50e5 1.89e5 6.27e5 2.22e5 7.10e5 ## 3 04 Ariz… 1.77e6 5.39e5 2.71e6 9.53e5 3.66e6 1.37e6 5.13e6 1.90e6 6.39e6 ## 4 05 Arka… 1.90e6 6.08e5 2.25e6 8.06e5 2.35e6 8.91e5 2.67e6 1.04e6 2.92e6 ## 5 06 Cali… 1.98e7 6.57e6 2.36e7 8.62e6 2.97e7 1.04e7 3.39e7 1.15e7 3.73e7 ## 6 08 Colo… 2.18e6 6.85e5 2.83e6 1.04e6 3.28e6 1.28e6 4.28e6 1.65e6 5.03e6 ## # … with 315 more variables: HHD10 &lt;dbl&gt;, `2020-01-21` &lt;dbl&gt;, ## # `2020-01-22` &lt;dbl&gt;, `2020-01-23` &lt;dbl&gt;, `2020-01-24` &lt;dbl&gt;, ## # `2020-01-25` &lt;dbl&gt;, `2020-01-26` &lt;dbl&gt;, `2020-01-27` &lt;dbl&gt;, ## # `2020-01-28` &lt;dbl&gt;, `2020-01-29` &lt;dbl&gt;, `2020-01-30` &lt;dbl&gt;, ## # `2020-01-31` &lt;dbl&gt;, `2020-02-01` &lt;dbl&gt;, `2020-02-02` &lt;dbl&gt;, ## # `2020-02-03` &lt;dbl&gt;, `2020-02-04` &lt;dbl&gt;, `2020-02-05` &lt;dbl&gt;, ## # `2020-02-06` &lt;dbl&gt;, `2020-02-07` &lt;dbl&gt;, `2020-02-08` &lt;dbl&gt;, ## # `2020-02-09` &lt;dbl&gt;, `2020-02-10` &lt;dbl&gt;, `2020-02-11` &lt;dbl&gt;, ## # `2020-02-12` &lt;dbl&gt;, `2020-02-13` &lt;dbl&gt;, `2020-02-14` &lt;dbl&gt;, ## # `2020-02-15` &lt;dbl&gt;, `2020-02-16` &lt;dbl&gt;, `2020-02-17` &lt;dbl&gt;, ## # `2020-02-18` &lt;dbl&gt;, `2020-02-19` &lt;dbl&gt;, `2020-02-20` &lt;dbl&gt;, ## # `2020-02-21` &lt;dbl&gt;, `2020-02-22` &lt;dbl&gt;, `2020-02-23` &lt;dbl&gt;, ## # `2020-02-24` &lt;dbl&gt;, `2020-02-25` &lt;dbl&gt;, `2020-02-26` &lt;dbl&gt;, ## # `2020-02-27` &lt;dbl&gt;, `2020-02-28` &lt;dbl&gt;, `2020-02-29` &lt;dbl&gt;, ## # `2020-03-01` &lt;dbl&gt;, `2020-03-02` &lt;dbl&gt;, `2020-03-03` &lt;dbl&gt;, ## # `2020-03-04` &lt;dbl&gt;, `2020-03-05` &lt;dbl&gt;, `2020-03-06` &lt;dbl&gt;, ## # `2020-03-07` &lt;dbl&gt;, `2020-03-08` &lt;dbl&gt;, `2020-03-09` &lt;dbl&gt;, ## # `2020-03-10` &lt;dbl&gt;, `2020-03-11` &lt;dbl&gt;, `2020-03-12` &lt;dbl&gt;, ## # `2020-03-13` &lt;dbl&gt;, `2020-03-14` &lt;dbl&gt;, `2020-03-15` &lt;dbl&gt;, ## # `2020-03-16` &lt;dbl&gt;, `2020-03-17` &lt;dbl&gt;, `2020-03-18` &lt;dbl&gt;, ## # `2020-03-19` &lt;dbl&gt;, `2020-03-20` &lt;dbl&gt;, `2020-03-21` &lt;dbl&gt;, ## # `2020-03-22` &lt;dbl&gt;, `2020-03-23` &lt;dbl&gt;, `2020-03-24` &lt;dbl&gt;, ## # `2020-03-25` &lt;dbl&gt;, `2020-03-26` &lt;dbl&gt;, `2020-03-27` &lt;dbl&gt;, ## # `2020-03-28` &lt;dbl&gt;, `2020-03-29` &lt;dbl&gt;, `2020-03-30` &lt;dbl&gt;, ## # `2020-03-31` &lt;dbl&gt;, `2020-04-01` &lt;dbl&gt;, `2020-04-02` &lt;dbl&gt;, ## # `2020-04-03` &lt;dbl&gt;, `2020-04-04` &lt;dbl&gt;, `2020-04-05` &lt;dbl&gt;, ## # `2020-04-06` &lt;dbl&gt;, `2020-04-07` &lt;dbl&gt;, `2020-04-08` &lt;dbl&gt;, ## # `2020-04-09` &lt;dbl&gt;, `2020-04-10` &lt;dbl&gt;, `2020-04-11` &lt;dbl&gt;, ## # `2020-04-12` &lt;dbl&gt;, `2020-04-13` &lt;dbl&gt;, `2020-04-14` &lt;dbl&gt;, ## # `2020-04-15` &lt;dbl&gt;, `2020-04-16` &lt;dbl&gt;, `2020-04-17` &lt;dbl&gt;, ## # `2020-04-18` &lt;dbl&gt;, `2020-04-19` &lt;dbl&gt;, `2020-04-20` &lt;dbl&gt;, ## # `2020-04-21` &lt;dbl&gt;, `2020-04-22` &lt;dbl&gt;, `2020-04-23` &lt;dbl&gt;, ## # `2020-04-24` &lt;dbl&gt;, `2020-04-25` &lt;dbl&gt;, `2020-04-26` &lt;dbl&gt;, ## # `2020-04-27` &lt;dbl&gt;, `2020-04-28` &lt;dbl&gt;, … Cleaning data COVID-19 cases data The COVID-19 cases data are in wide format, with individual columns for each day’s case counts. To visualize and analyze the data, it will be much easier to reshape the data so that it is organized in long format, with a single column for case counts and another column indicating the date those counts are associated with. In addition, it will be useful to derive some time-related variables (e.g., day of year, week of year) from the dates. Finally, we should transform our cumulative case counts into regular counts and create some rate variables by normalizing by population count. US_cases_long &lt;- US_cases %&gt;% # select columns of interest select(fips, NAME, POP10, matches(&quot;^\\\\d&quot;)) %&gt;% # rename some columns rename(GEOID = fips, state = NAME, pop_count_2010 = POP10) %&gt;% # reshape to long format for dates pivot_longer(cols = grep(&quot;^\\\\d&quot;, colnames(.), value = TRUE), names_to = &quot;date&quot;, values_to = &quot;cases_cum&quot;) %&gt;% # create new derived time variables from dates mutate(date = ymd(date), # year-month-day format day_of_year = yday(date), week_of_year = week(date), month = month(date)) %&gt;% group_by(state) %&gt;% # create cases counts mutate(cases_count = cases_cum - lag(cases_cum, default = 0), # tidy-up negative counts cases_count_pos = ifelse(cases_count &lt; 0, 0, cases_count), # create cases rates cases_rate_100K = (cases_count_pos / pop_count_2010) * 1e5, cases_cum_rate_100K = (cases_cum / pop_count_2010) * 1e5) glimpse(US_cases_long) # 16014 observations (50 states + 1 DC * 314 days) ## Rows: 16,014 ## Columns: 12 ## Groups: state [51] ## $ GEOID &lt;chr&gt; &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;… ## $ state &lt;chr&gt; &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alab… ## $ pop_count_2010 &lt;dbl&gt; 4779736, 4779736, 4779736, 4779736, 4779736, 4779… ## $ date &lt;date&gt; 2020-01-21, 2020-01-22, 2020-01-23, 2020-01-24, … ## $ cases_cum &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ day_of_year &lt;dbl&gt; 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 3… ## $ week_of_year &lt;dbl&gt; 3, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 6, 6… ## $ month &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2… ## $ cases_count &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ cases_count_pos &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ cases_rate_100K &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ cases_cum_rate_100K &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… Aggregate data The cleaned data object US_cases_long has 16,014 observations (50 states + 1 DC * 314 days). For visualization, this should be fine in most cases. When we come to build models for these data, they may take a long time to run. If we’re mainly interested in longer term trends, we can probably get a good approximation by aggregating the data to the weekly level for modeling: # aggregate to weekly level (for later modeling) US_cases_long_week &lt;- US_cases_long %&gt;% group_by(GEOID, state, week_of_year) %&gt;% summarize(pop_count_2010 = mean(pop_count_2010), cases_count_pos = sum(cases_count_pos), cases_rate_100K = sum(cases_rate_100K)) %&gt;% drop_na() glimpse(US_cases_long_week) ## Rows: 2,346 ## Columns: 6 ## Groups: GEOID, state [51] ## $ GEOID &lt;chr&gt; &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;,… ## $ state &lt;chr&gt; &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;… ## $ week_of_year &lt;dbl&gt; 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, … ## $ pop_count_2010 &lt;dbl&gt; 4779736, 4779736, 4779736, 4779736, 4779736, 4779736,… ## $ cases_count_pos &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 39, 203, 757, 1198, 1756, 137… ## $ cases_rate_100K &lt;dbl&gt; 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000… "],
["optional-u-s-census-data-r.html", "OPTIONAL: U.S. Census data (R) U.S. Census Bureau API Clean U.S. Census data Combine Census and COVID-19 data Aggregate to weekly-level", " OPTIONAL: U.S. Census data (R) This section is optional. It provides an example of how to acquire potentially interesting predictors of COVID-19 cases from the U.S. Census Bureau. The COVID-19 dataset we accessed above provides daily COVID-19 case counts for each U.S State, together with population counts from the 2010 Decennial Census. This should be enough information to produce some interesting visualizations. For modeling, however, we really only have one useful predictor in the dataset — time. This section describes some options for acquiring other potentially interesting predictors of COVID-19 cases. U.S. Census Bureau API We may want to use additional demographic information in our visualizations and analysis of the COVID-19 cases. An obvious place to source this information is from the U.S. Census Bureau. There are three U.S. Census Bureau data sources, each with their own API: Decennial Census: survey of every household in the U.S. every 10 years — used to calculate population of U.S. geographic areas. American Community Survey: yearly representative sample of 3.5 million households — used to calculate population estimates of U.S. geographic areas. Population Estimates: yearly population estimates of U.S. geographic areas. The COVID-19 data from Dataverse already contains population values from the 2010 decennial census. But, using the Census Bureau’s Population Estimates API, we can get updated population data for 2019 as well as population data stratified by age groups, race, and sex. We’re going to use the tidycensus package as an interface to the Census Bureau API. A basic usage guide is available — https://walker-data.com/tidycensus/articles/basic-usage.html — but we’ll walk through all the necessary steps. The first step is to sign-up for an API key: http://api.census.gov/data/key_signup.html. Then give the key a name. # store API key API_key &lt;- &quot;your-API-key-here&quot; We can then set the API key for our current R session using the census_api_key() function (or we can include it in an .Renviron file for future use): # set API key for current session census_api_key(API_key) Next, we can use the get_estimates() function to access the Population Estimates API and extract variables of interest: pop &lt;- get_estimates( geography = &quot;state&quot;, # we&#39;ll select state-level data product = &quot;population&quot;, # provides overall population estimates and population densities year = 2019, # the latest year available key = API_key) glimpse(pop) ## Rows: 104 ## Columns: 4 ## $ NAME &lt;chr&gt; &quot;Mississippi&quot;, &quot;Missouri&quot;, &quot;Montana&quot;, &quot;Nebraska&quot;, &quot;Nevada&quot;, … ## $ GEOID &lt;chr&gt; &quot;28&quot;, &quot;29&quot;, &quot;30&quot;, &quot;31&quot;, &quot;32&quot;, &quot;33&quot;, &quot;34&quot;, &quot;35&quot;, &quot;36&quot;, &quot;37&quot;, … ## $ variable &lt;chr&gt; &quot;POP&quot;, &quot;POP&quot;, &quot;POP&quot;, &quot;POP&quot;, &quot;POP&quot;, &quot;POP&quot;, &quot;POP&quot;, &quot;POP&quot;, &quot;POP… ## $ value &lt;dbl&gt; 2976149, 6137428, 1068778, 1934408, 3080156, 1359711, 888219… Get population estimates by age group: age &lt;- get_estimates( geography = &quot;state&quot;, product = &quot;characteristics&quot;, # provides population estimates stratified by the variable specified in `breakdown` breakdown = &quot;AGEGROUP&quot;, # population estimates for different age groups breakdown_labels = TRUE, # labels for age groups year = 2019, key = API_key) glimpse(age) ## Rows: 1,664 ## Columns: 4 ## $ GEOID &lt;chr&gt; &quot;28&quot;, &quot;28&quot;, &quot;28&quot;, &quot;28&quot;, &quot;28&quot;, &quot;28&quot;, &quot;28&quot;, &quot;28&quot;, &quot;28&quot;, &quot;28&quot;, … ## $ NAME &lt;chr&gt; &quot;Mississippi&quot;, &quot;Mississippi&quot;, &quot;Mississippi&quot;, &quot;Mississippi&quot;, … ## $ value &lt;dbl&gt; 2976149, 183478, 189377, 206282, 201350, 201517, 206989, 186… ## $ AGEGROUP &lt;fct&gt; All ages, Age 0 to 4 years, Age 5 to 9 years, Age 10 to 14 y… Get population estimates by sex: sex &lt;- get_estimates( geography = &quot;state&quot;, product = &quot;characteristics&quot;, breakdown = &quot;SEX&quot;, # population estimates for different sexes breakdown_labels = TRUE, year = 2019, key = API_key) glimpse(sex) ## Rows: 156 ## Columns: 4 ## $ GEOID &lt;chr&gt; &quot;28&quot;, &quot;28&quot;, &quot;28&quot;, &quot;29&quot;, &quot;29&quot;, &quot;29&quot;, &quot;30&quot;, &quot;30&quot;, &quot;31&quot;, &quot;32&quot;, &quot;32… ## $ NAME &lt;chr&gt; &quot;Mississippi&quot;, &quot;Mississippi&quot;, &quot;Mississippi&quot;, &quot;Missouri&quot;, &quot;Misso… ## $ value &lt;dbl&gt; 2976149, 1442292, 1533857, 6137428, 3012662, 3124766, 1068778, … ## $ SEX &lt;chr&gt; &quot;Both sexes&quot;, &quot;Male&quot;, &quot;Female&quot;, &quot;Both sexes&quot;, &quot;Male&quot;, &quot;Female&quot;,… Get population estimates by race: race &lt;- get_estimates( geography = &quot;state&quot;, product = &quot;characteristics&quot;, breakdown =&quot;RACE&quot;, # population estimates for different races breakdown_labels = TRUE, year = 2019, key = API_key) glimpse(race) ## Rows: 613 ## Columns: 4 ## $ GEOID &lt;chr&gt; &quot;28&quot;, &quot;28&quot;, &quot;28&quot;, &quot;28&quot;, &quot;28&quot;, &quot;28&quot;, &quot;28&quot;, &quot;28&quot;, &quot;28&quot;, &quot;28&quot;, &quot;28… ## $ NAME &lt;chr&gt; &quot;Mississippi&quot;, &quot;Mississippi&quot;, &quot;Mississippi&quot;, &quot;Mississippi&quot;, &quot;Mi… ## $ value &lt;dbl&gt; 2976149, 1758081, 1124559, 18705, 33032, 1806, 39966, 1792535, … ## $ RACE &lt;chr&gt; &quot;All races&quot;, &quot;White alone&quot;, &quot;Black alone&quot;, &quot;American Indian and… Clean U.S. Census data The Census data we extracted contain population estimates for multiple categories of age, race, and sex. It will be useful to simplify these data by creating some derived variables that may be of interest when visualizing and analyzing the data. For example, for each state, we may want to calculate: Overall population count and density Proportion of people that are 65 years and older Proportion of people that are female (or male) Proportion of people that are black (or white, or other race) Overall population estimates: pop_wide &lt;- pop %&gt;% # order by GEOID (same as state FIPS code) arrange(GEOID) %&gt;% # rename state rename(state = NAME) %&gt;% # exclude Puerto Rico filter(state != &quot;Puerto Rico&quot;) %&gt;% # reshape population variables to wide format pivot_wider(names_from = variable, values_from = value) %&gt;% # rename population variables rename(pop_count_2019 = POP, pop_density_2019 = DENSITY) glimpse(pop_wide) ## Rows: 51 ## Columns: 4 ## $ state &lt;chr&gt; &quot;Alabama&quot;, &quot;Alaska&quot;, &quot;Arizona&quot;, &quot;Arkansas&quot;, &quot;Califor… ## $ GEOID &lt;chr&gt; &quot;01&quot;, &quot;02&quot;, &quot;04&quot;, &quot;05&quot;, &quot;06&quot;, &quot;08&quot;, &quot;09&quot;, &quot;10&quot;, &quot;11&quot;… ## $ pop_count_2019 &lt;dbl&gt; 4903185, 731545, 7278717, 3017804, 39512223, 5758736… ## $ pop_density_2019 &lt;dbl&gt; 96.811652, 1.281127, 64.043252, 57.992836, 253.52068… Population estimates by age group: age_wide &lt;- age %&gt;% # order by GEOID (same as state FIPS code) arrange(GEOID) %&gt;% # rename state rename(state = NAME) %&gt;% # reshape the age groups to wide format pivot_wider(names_from = AGEGROUP, values_from = value) %&gt;% # create variable for percentortion of people that are 65 years and older mutate(percent_age65over = (`65 years and over` / `All ages`) * 100) %&gt;% # select columns of interest select(GEOID, state, percent_age65over) glimpse(age_wide) ## Rows: 52 ## Columns: 3 ## $ GEOID &lt;chr&gt; &quot;01&quot;, &quot;02&quot;, &quot;04&quot;, &quot;05&quot;, &quot;06&quot;, &quot;08&quot;, &quot;09&quot;, &quot;10&quot;, &quot;11… ## $ state &lt;chr&gt; &quot;Alabama&quot;, &quot;Alaska&quot;, &quot;Arizona&quot;, &quot;Arkansas&quot;, &quot;Califo… ## $ percent_age65over &lt;dbl&gt; 17.33235, 12.51980, 17.97890, 17.35971, 14.77547, 1… Population estimates by sex: sex_wide &lt;- sex %&gt;% # order by GEOID (same as state FIPS code) arrange(GEOID) %&gt;% # rename state rename(state = NAME) %&gt;% # reshape the sex categories to wide format pivot_wider(names_from = SEX, values_from = value) %&gt;% # create variable for percentortion of people that are female mutate(percent_female = (Female / `Both sexes`) * 100) %&gt;% # select columns of interest select(GEOID, state, percent_female) glimpse(sex_wide) ## Rows: 52 ## Columns: 3 ## $ GEOID &lt;chr&gt; &quot;01&quot;, &quot;02&quot;, &quot;04&quot;, &quot;05&quot;, &quot;06&quot;, &quot;08&quot;, &quot;09&quot;, &quot;10&quot;, &quot;11&quot;, … ## $ state &lt;chr&gt; &quot;Alabama&quot;, &quot;Alaska&quot;, &quot;Arizona&quot;, &quot;Arkansas&quot;, &quot;Californi… ## $ percent_female &lt;dbl&gt; 51.67392, 47.86131, 50.30310, 50.90417, 50.28158, 49.6… Population estimates by race: race_wide &lt;- race %&gt;% # order by GEOID (same as state FIPS code) arrange(GEOID) %&gt;% # rename state rename(state = NAME) %&gt;% # reshape the race categories to wide format pivot_wider(names_from = RACE, values_from = value) %&gt;% # create variables for percentortion of people that are black and white mutate(percent_white = (`White alone` / `All races`) * 100, percent_black = (`Black alone` / `All races`) * 100) %&gt;% # select columns of interest select(GEOID, state, percent_white, percent_black) glimpse(race_wide) ## Rows: 52 ## Columns: 4 ## $ GEOID &lt;chr&gt; &quot;01&quot;, &quot;02&quot;, &quot;04&quot;, &quot;05&quot;, &quot;06&quot;, &quot;08&quot;, &quot;09&quot;, &quot;10&quot;, &quot;11&quot;, &quot;… ## $ state &lt;chr&gt; &quot;Alabama&quot;, &quot;Alaska&quot;, &quot;Arizona&quot;, &quot;Arkansas&quot;, &quot;California… ## $ percent_white &lt;dbl&gt; 69.12641, 65.27117, 82.61679, 79.03953, 71.93910, 86.93… ## $ percent_black &lt;dbl&gt; 26.7844473, 3.7055820, 5.1794430, 15.6752393, 6.4606767… We can now merge all the cleaned Census data into one object called demographics: demographics &lt;- list(pop_wide, age_wide, sex_wide, race_wide) %&gt;% reduce(left_join, by = c(&quot;GEOID&quot;, &quot;state&quot;)) glimpse(demographics) ## Rows: 51 ## Columns: 8 ## $ state &lt;chr&gt; &quot;Alabama&quot;, &quot;Alaska&quot;, &quot;Arizona&quot;, &quot;Arkansas&quot;, &quot;Califo… ## $ GEOID &lt;chr&gt; &quot;01&quot;, &quot;02&quot;, &quot;04&quot;, &quot;05&quot;, &quot;06&quot;, &quot;08&quot;, &quot;09&quot;, &quot;10&quot;, &quot;11… ## $ pop_count_2019 &lt;dbl&gt; 4903185, 731545, 7278717, 3017804, 39512223, 575873… ## $ pop_density_2019 &lt;dbl&gt; 96.811652, 1.281127, 64.043252, 57.992836, 253.5206… ## $ percent_age65over &lt;dbl&gt; 17.33235, 12.51980, 17.97890, 17.35971, 14.77547, 1… ## $ percent_female &lt;dbl&gt; 51.67392, 47.86131, 50.30310, 50.90417, 50.28158, 4… ## $ percent_white &lt;dbl&gt; 69.12641, 65.27117, 82.61679, 79.03953, 71.93910, 8… ## $ percent_black &lt;dbl&gt; 26.7844473, 3.7055820, 5.1794430, 15.6752393, 6.460… Combine Census and COVID-19 data Merge the COVID-19 cases data with Census demographic data: # merge COVID-19 cases with demographics US_cases_long_demogr &lt;- US_cases_long %&gt;% left_join(demographics, by = c(&quot;GEOID&quot;, &quot;state&quot;)) # update the case rate variables to use population estimates from 2019 US_cases_long_demogr &lt;- US_cases_long_demogr %&gt;% mutate(cases_cum_rate_100K = (cases_cum / pop_count_2019) * 1e5, cases_rate_100K = (cases_count_pos / pop_count_2019) * 1e5) glimpse(US_cases_long_demogr) ## Rows: 16,014 ## Columns: 18 ## Groups: state [51] ## $ GEOID &lt;chr&gt; &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;… ## $ state &lt;chr&gt; &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alab… ## $ pop_count_2010 &lt;dbl&gt; 4779736, 4779736, 4779736, 4779736, 4779736, 4779… ## $ date &lt;date&gt; 2020-01-21, 2020-01-22, 2020-01-23, 2020-01-24, … ## $ cases_cum &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ day_of_year &lt;dbl&gt; 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 3… ## $ week_of_year &lt;dbl&gt; 3, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 6, 6… ## $ month &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2… ## $ cases_count &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ cases_count_pos &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ cases_rate_100K &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ cases_cum_rate_100K &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ pop_count_2019 &lt;dbl&gt; 4903185, 4903185, 4903185, 4903185, 4903185, 4903… ## $ pop_density_2019 &lt;dbl&gt; 96.81165, 96.81165, 96.81165, 96.81165, 96.81165,… ## $ percent_age65over &lt;dbl&gt; 17.33235, 17.33235, 17.33235, 17.33235, 17.33235,… ## $ percent_female &lt;dbl&gt; 51.67392, 51.67392, 51.67392, 51.67392, 51.67392,… ## $ percent_white &lt;dbl&gt; 69.12641, 69.12641, 69.12641, 69.12641, 69.12641,… ## $ percent_black &lt;dbl&gt; 26.78445, 26.78445, 26.78445, 26.78445, 26.78445,… Aggregate to weekly-level Once again, for the purposes of modeling, it may be useful to aggregate to the weekly-level: # COVID-19 data and demographic data US_cases_long_demogr_week &lt;- US_cases_long_demogr %&gt;% group_by(state, week_of_year) %&gt;% summarize(pop_count_2019 = mean(pop_count_2019), percent_age65over = mean(percent_age65over), percent_female = mean(percent_female), percent_white = mean(percent_white), percent_black = mean(percent_black), cases_count_pos = sum(cases_count_pos), cases_rate_100K = sum(cases_rate_100K)) %&gt;% drop_na() Let’s store the data frame in a binary R file so that we can easily access it later: save(US_cases_long_demogr_week, file = &quot;data_r/US_cases_long_demogr_week.Rdata&quot;) "],
["day-2-data-visualization-r.html", "DAY 2: Data visualization (R) Non-spatial graphs Static Maps Interactive Maps", " DAY 2: Data visualization (R) The COVID-19 cases data we have are inherently temporal and spatial. Let’s explore the space and time dimensions of the case data through visualization. Non-spatial graphs We can easily create a wide range of non-spatial (and spatial) graphs using the ggplot() function from the ggplot2 package. If you need a refresher on this package, both IQSS and HBS collaborate on delivering a workshop devoted to ggplot2 each semester and the workshop materials can be accessed here: https://iqss.github.io/dss-workshops/Rgraphics.html. We can start with a very simple line graph of the COVID-19 cases rates over time: # line graph of covid cases rates ggplot(US_cases_long, aes(x = date, y = cases_rate_100K)) + geom_line() + theme_classic() This gives us an overall sense that the rate of cases has increased over time and has become particularly prevalent in the fall of 2020. But, because the lines for each state are not discernible, we can’t see if some states have a different trajectory of case rates than other states. We could try making each state’s line a different color, but with 50 states plus D.C., we won’t be able to easily identify which color hue is associated with which state. A better solution is to use faceting to produce mini-plots for each state. Let’s create a new line graph of COVID-19 cases rates over time, this time with a separate mini-plot for each state: # line graphs of covid cases rates for each state ggplot(US_cases_long, aes(x = date, y = cases_rate_100K)) + geom_line() + facet_wrap(~ state, scales = &quot;free_y&quot;) + # make the y-axis independent for each state theme_classic() We can try the same strategy for cumulative COVID-19 case rates over time. First, in a graph that jumbles together all the states: # line graph of cumulative covid cases rates ggplot(US_cases_long, aes(x = date, y = cases_cum_rate_100K)) + geom_line() + theme_classic() Again, we get a sense of the overall trend here, but we can get a much better picture of state-level differences by faceting. So, let’s create a new line graph of COVID-19 cumulative cases rates over time, this time with a separate mini-plot for each state: # line graphs of cumulative covid cases rates for each state ggplot(US_cases_long, aes(x = date, y = cases_cum_rate_100K)) + geom_line() + facet_wrap(~ state, scales = &quot;free_y&quot;) + theme_classic() Static Maps A great way to visualize spatial relationships in data is to superimpose variables onto a map. For some datasets, this could involve superimposing points or lines. For our state-level data, this will involve coloring state polygons in proportion to a variable of interest that represents an aggregate summary of a geographic characteristic within each state. Such a graph is often referred to as a choropleth map. To create a choropleth map we first need to acquire shapefiles that contain spatial data about U.S. state-level geographies. We can use the tigris package to get Census Tiger shapefiles for census geographies. In particular, we can use the states() function to get state-level geographies, and coastal boundaries can be gathered with the argument cb = TRUE: # download state-level census geographies us_state_geo &lt;- tigris::states(class = &quot;sf&quot;, cb = TRUE) %&gt;% # rename `NAME` variable to `state` rename(state = NAME) glimpse(us_state_geo) ## Rows: 56 ## Columns: 10 ## $ STATEFP &lt;chr&gt; &quot;12&quot;, &quot;78&quot;, &quot;30&quot;, &quot;27&quot;, &quot;24&quot;, &quot;45&quot;, &quot;23&quot;, &quot;15&quot;, &quot;11&quot;, &quot;69&quot;, … ## $ STATENS &lt;chr&gt; &quot;00294478&quot;, &quot;01802710&quot;, &quot;00767982&quot;, &quot;00662849&quot;, &quot;01714934&quot;, … ## $ AFFGEOID &lt;chr&gt; &quot;0400000US12&quot;, &quot;0400000US78&quot;, &quot;0400000US30&quot;, &quot;0400000US27&quot;, … ## $ GEOID &lt;chr&gt; &quot;12&quot;, &quot;78&quot;, &quot;30&quot;, &quot;27&quot;, &quot;24&quot;, &quot;45&quot;, &quot;23&quot;, &quot;15&quot;, &quot;11&quot;, &quot;69&quot;, … ## $ STUSPS &lt;chr&gt; &quot;FL&quot;, &quot;VI&quot;, &quot;MT&quot;, &quot;MN&quot;, &quot;MD&quot;, &quot;SC&quot;, &quot;ME&quot;, &quot;HI&quot;, &quot;DC&quot;, &quot;MP&quot;, … ## $ state &lt;chr&gt; &quot;Florida&quot;, &quot;United States Virgin Islands&quot;, &quot;Montana&quot;, &quot;Minne… ## $ LSAD &lt;chr&gt; &quot;00&quot;, &quot;00&quot;, &quot;00&quot;, &quot;00&quot;, &quot;00&quot;, &quot;00&quot;, &quot;00&quot;, &quot;00&quot;, &quot;00&quot;, &quot;00&quot;, … ## $ ALAND &lt;dbl&gt; 138947364717, 348021896, 376966832749, 206230065476, 2515172… ## $ AWATER &lt;dbl&gt; 31362872853, 1550236199, 3869031338, 18942261495, 6979340970… ## $ geometry &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((-80.17628 2..., MULTIPOLYGON ((… We can now merge the spatial data with our weekly COVID-19 cases data, keeping only the contiguous 48 states (plus D.C.): # merge weekly COVID-19 cases with spatial data US_cases_long_week_spatial &lt;- us_state_geo %&gt;% left_join(US_cases_long_week, by = c(&quot;GEOID&quot;, &quot;state&quot;)) %&gt;% filter(GEOID &lt; 60 &amp; state != &quot;Alaska&quot; &amp; state != &quot;Hawaii&quot;) glimpse(US_cases_long_week_spatial) ## Rows: 2,254 ## Columns: 14 ## $ STATEFP &lt;chr&gt; &quot;12&quot;, &quot;12&quot;, &quot;12&quot;, &quot;12&quot;, &quot;12&quot;, &quot;12&quot;, &quot;12&quot;, &quot;12&quot;, &quot;12&quot;,… ## $ STATENS &lt;chr&gt; &quot;00294478&quot;, &quot;00294478&quot;, &quot;00294478&quot;, &quot;00294478&quot;, &quot;0029… ## $ AFFGEOID &lt;chr&gt; &quot;0400000US12&quot;, &quot;0400000US12&quot;, &quot;0400000US12&quot;, &quot;0400000… ## $ GEOID &lt;chr&gt; &quot;12&quot;, &quot;12&quot;, &quot;12&quot;, &quot;12&quot;, &quot;12&quot;, &quot;12&quot;, &quot;12&quot;, &quot;12&quot;, &quot;12&quot;,… ## $ STUSPS &lt;chr&gt; &quot;FL&quot;, &quot;FL&quot;, &quot;FL&quot;, &quot;FL&quot;, &quot;FL&quot;, &quot;FL&quot;, &quot;FL&quot;, &quot;FL&quot;, &quot;FL&quot;,… ## $ state &lt;chr&gt; &quot;Florida&quot;, &quot;Florida&quot;, &quot;Florida&quot;, &quot;Florida&quot;, &quot;Florida&quot;… ## $ LSAD &lt;chr&gt; &quot;00&quot;, &quot;00&quot;, &quot;00&quot;, &quot;00&quot;, &quot;00&quot;, &quot;00&quot;, &quot;00&quot;, &quot;00&quot;, &quot;00&quot;,… ## $ ALAND &lt;dbl&gt; 138947364717, 138947364717, 138947364717, 13894736471… ## $ AWATER &lt;dbl&gt; 31362872853, 31362872853, 31362872853, 31362872853, 3… ## $ week_of_year &lt;dbl&gt; 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, … ## $ pop_count_2010 &lt;dbl&gt; 18801310, 18801310, 18801310, 18801310, 18801310, 188… ## $ cases_count_pos &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 3, 20, 187, 1257, 5275, 7997, 6881,… ## $ cases_rate_100K &lt;dbl&gt; 0.00000000, 0.00000000, 0.00000000, 0.00000000, 0.000… ## $ geometry &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((-80.17628 2..., MULTIPOL… Let’s create a choropleth map for the latest week’s COVID-19 cases using ggplot(): US_cases_long_week_spatial %&gt;% # subset data for only latest week filter(week_of_year == max(week_of_year, na.rm = TRUE)) %&gt;% # map starts here ggplot(aes(fill = cases_rate_100K, color = cases_rate_100K)) + geom_sf() + coord_sf(crs = 5070, datum = NA) + scale_fill_viridis(direction = -1, name = &quot;Case rate\\n(per 100K population)&quot;) + scale_color_viridis(direction = -1, name = &quot;Case rate\\n(per 100K population)&quot;) + labs(title = &quot;COVID-19 case rates for last week&quot;, caption = &quot;Data Sources: Harvard Dataverse, 2020; U.S. Census Bureau, 2019&quot;) Interactive Maps Static maps are great for publications. Interactive maps, which can be viewed in a browser, can potentially provide a much richer source of information. A good overview of the mapping functionality in R is provided here: https://map-rfun.library.duke.edu/index.html. In this section, we’ll focus on building a simple interactive map using the mapview package, which is a data-driven API for the leaflet package. # set some options for the graph mapviewOptions(fgb = FALSE, # set to FALSE to embed data directly into the HTML leafletWidth = 800, legend.pos = &quot;bottomright&quot;) # create map USmap &lt;- US_cases_long_week_spatial %&gt;% # subset data for only latest week filter(week_of_year == max(week_of_year, na.rm = TRUE)) %&gt;% # map starts here mapview(zcol = &quot;cases_rate_100K&quot;, layer.name = &quot;Case rates (per 100K)&quot;) # print map USmap@map "],
["day-3-data-analysis-r.html", "DAY 3: Data analysis (R) Descriptives Modeling", " DAY 3: Data analysis (R) In this section, we will be exploring the relationships between COVID-19 cases and demographic data from the Census Bureau. If you did not complete the optional Census data section, you can still access these data by loading the following file: load(&quot;data_r/US_cases_long_demogr_week.Rdata&quot;) glimpse(US_cases_long_demogr_week) ## Rows: 2,346 ## Columns: 9 ## Groups: state [51] ## $ state &lt;chr&gt; &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabam… ## $ week_of_year &lt;dbl&gt; 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17… ## $ pop_count_2019 &lt;dbl&gt; 4903185, 4903185, 4903185, 4903185, 4903185, 490318… ## $ percent_age65over &lt;dbl&gt; 17.33235, 17.33235, 17.33235, 17.33235, 17.33235, 1… ## $ percent_female &lt;dbl&gt; 51.67392, 51.67392, 51.67392, 51.67392, 51.67392, 5… ## $ percent_white &lt;dbl&gt; 69.12641, 69.12641, 69.12641, 69.12641, 69.12641, 6… ## $ percent_black &lt;dbl&gt; 26.78445, 26.78445, 26.78445, 26.78445, 26.78445, 2… ## $ cases_count_pos &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 39, 203, 757, 1198, 1756, 1… ## $ cases_rate_100K &lt;dbl&gt; 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.00000… Descriptives It’s always a good idea to start data analysis by looking at some descriptive statistics of the sample data. Here, we can inspect the demographic data we accessed through the Census API: US_cases_long_demogr_week %&gt;% group_by(state) %&gt;% summarize_at(vars(percent_age65over, percent_female, percent_white, percent_black), .funs = mean) %&gt;% mutate_if(is.numeric, round, 1) ## # A tibble: 51 x 5 ## state percent_age65ov… percent_female percent_white percent_black ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Alabama 17.3 51.7 69.1 26.8 ## 2 Alaska 12.5 47.9 65.3 3.7 ## 3 Arizona 18 50.3 82.6 5.2 ## 4 Arkansas 17.4 50.9 79 15.7 ## 5 California 14.8 50.3 71.9 6.5 ## 6 Colorado 14.6 49.6 86.9 4.6 ## 7 Connecticut 17.7 51.2 79.7 12.2 ## 8 Delaware 19.4 51.7 69.2 23.2 ## 9 District of Colu… 12.4 52.6 46 46 ## 10 Florida 20.9 51.1 77.3 16.9 ## # … with 41 more rows Modeling The data we have consists of counts of COVID-19 cases over time for each of 50 U.S. states and D.C. These data will be challenging to model, since we will have to deal with the following issues: The response consists of counts with a huge number of zeros and an extended right tail. Typically, to model counts we’d use a poisson model. Here, the extended right tail suggests the data are overdispersed (i.e., the variance is greater than the mean), which would mean the restrictive assumptions of the poisson distribution are not met and may push us towards a quasi-poisson or negative binomial model. In addition, we may need some machinery in the model to deal with the excess of zeros (a zero-inflation component), since this is atypical for a poisson or negative binomial model. Let’s inspect the response variable: # response summary summary(US_cases_long_demogr_week$cases_count_pos) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0 175.5 2001.0 5695.7 6093.2 94909.0 # marginal response distribution (truncated to counts &lt; 1000) ggplot(US_cases_long_demogr_week %&gt;% filter(cases_count_pos &lt; 1000), aes(x = cases_count_pos)) + geom_histogram(bins = 1e3) + theme_classic() The data are inherently spatial in nature — in this case, at the state-level. The data are inherently temporal in nature — in this case, at the daily- or weekly-level. Cross-sectional models Let’s start with something at the simpler end of the scale. We can reduce complexity by initially modeling a single time point (for example, the most recent week of case data), with a subset of states, and just a single predictor — the intercept — to estimate the average number of cases. # filter the most recent week&#39;s data US_cases_latest_week &lt;- US_cases_long_demogr_week %&gt;% filter(week_of_year == max(week_of_year)) glimpse(US_cases_latest_week) ## Rows: 51 ## Columns: 9 ## Groups: state [51] ## $ state &lt;chr&gt; &quot;Alabama&quot;, &quot;Alaska&quot;, &quot;Arizona&quot;, &quot;Arkansas&quot;, &quot;Califo… ## $ week_of_year &lt;dbl&gt; 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48,… ## $ pop_count_2019 &lt;dbl&gt; 4903185, 731545, 7278717, 3017804, 39512223, 575873… ## $ percent_age65over &lt;dbl&gt; 17.33235, 12.51980, 17.97890, 17.35971, 14.77547, 1… ## $ percent_female &lt;dbl&gt; 51.67392, 47.86131, 50.30310, 50.90417, 50.28158, 4… ## $ percent_white &lt;dbl&gt; 69.12641, 65.27117, 82.61679, 79.03953, 71.93910, 8… ## $ percent_black &lt;dbl&gt; 26.7844473, 3.7055820, 5.1794430, 15.6752393, 6.460… ## $ cases_count_pos &lt;dbl&gt; 10364, 3188, 18052, 7935, 66491, 22175, 5301, 2587,… ## $ cases_rate_100K &lt;dbl&gt; 211.37281, 435.79001, 248.01074, 262.93954, 168.279… Now let’s inspect the response variable for just this last week of data: # histogram of last week&#39;s counts ggplot(US_cases_latest_week, aes(x = cases_count_pos)) + geom_histogram(bins = 50) + theme_classic() # distribution of cases in sample summary(US_cases_latest_week$cases_count_pos) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 338 5106 10364 15163 20009 66491 Usually with count data, we’d fit a model designed to deal with the idiosyncrasies of counts — which are integer-only, lower bounded at zero, and generally heavily right skewed — such as a poisson, quasi-poisson, or negative binomial model. Here, however, the average number of counts is high and we don’t have any observations near the theoretical lower boundary of zero, so we can try a basic linear model since in this situation the Gaussian family of distributions approximates the poisson. # fit intercept-only OLS model model_last_week1 &lt;- lm(cases_count_pos ~ 1, data = US_cases_latest_week) # inference summary(model_last_week1) ## ## Call: ## lm(formula = cases_count_pos ~ 1, data = US_cases_latest_week) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14825 -10058 -4799 4846 51328 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 15163 1981 7.655 0.00000000057 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 14150 on 50 degrees of freedom confint(model_last_week1) ## 2.5 % 97.5 % ## (Intercept) 11184.68 19141.6 # model diagnostics - simulate residuals using the DHARMa package model_last_week1_simres &lt;- simulateResiduals(model_last_week1) plot(model_last_week1_simres, quantreg = TRUE) We recovered the average number of cases for the latest week, pooled over all the states. Now we can try adding some of our explanatory variables. # fit OLS model model_last_week2 &lt;- lm(cases_count_pos ~ 1 + percent_age65over + percent_female + percent_black, data = US_cases_latest_week) # inference summary(model_last_week2) ## ## Call: ## lm(formula = cases_count_pos ~ 1 + percent_age65over + percent_female + ## percent_black, data = US_cases_latest_week) ## ## Residuals: ## Min 1Q Median 3Q Max ## -23275 -7096 -4382 3385 46178 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -312043.4 204588.9 -1.525 0.1339 ## percent_age65over -2358.0 1242.6 -1.898 0.0639 . ## percent_female 7363.1 4334.8 1.699 0.0960 . ## percent_black -469.0 341.1 -1.375 0.1756 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 13970 on 47 degrees of freedom ## Multiple R-squared: 0.08298,\tAdjusted R-squared: 0.02445 ## F-statistic: 1.418 on 3 and 47 DF, p-value: 0.2494 # model diagnostics - simulate residuals using the DHARMa package model_last_week2_simres &lt;- simulateResiduals(model_last_week2) plot(model_last_week2_simres, quantreg = TRUE) We’re not able to detect any effects of interest here — perhaps because we’re only using one week of data. We actually have a year’s worth of data, so let’s try modeling this as a panel (a longitudinal dataset). Panel models We have case count data for each state, tracked at the weekly-level for a year. This means that the data are clustered at the state-level (i.e., observations within states are likely to be correlated with one another more than observations between different states). We could deal with this clustering in several different ways, but using a multi-level model with random intercepts grouped by state is a good, flexible option. Let’s start with a linear model. model_panel1 &lt;- glmmTMB(cases_count_pos ~ 1 + week_of_year + percent_age65over + percent_female + percent_black + (1 | state), family = gaussian(link = &quot;identity&quot;), data = US_cases_long_demogr_week) # inference summary(model_panel1) ## Family: gaussian ( identity ) ## Formula: ## cases_count_pos ~ 1 + week_of_year + percent_age65over + percent_female + ## percent_black + (1 | state) ## Data: US_cases_long_demogr_week ## ## AIC BIC logLik deviance df.resid ## NA NA NA NA 2339 ## ## Random effects: ## ## Conditional model: ## Groups Name Variance Std.Dev. ## state (Intercept) 0.002297 0.04793 ## Residual 91426064.955941 9561.69781 ## Number of obs: 2346, groups: state, 51 ## ## Dispersion estimate for gaussian family (sigma^2): 9.14e+07 ## ## Conditional model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 2.023 20906.193 0.000 1.000 ## week_of_year 327.253 14.870 22.008 &lt; 0.0000000000000002 *** ## percent_age65over -597.360 125.788 -4.749 0.00000204 *** ## percent_female 135.539 442.844 0.306 0.760 ## percent_black 49.035 34.700 1.413 0.158 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # model diagnostics - simulate residuals using the DHARMa package model_panel1_simres &lt;- simulateResiduals(model_panel1) par(mfrow = c(1, 1)) plot(model_panel1_simres, quantreg = TRUE) Aside from the convergence warning, the model diagnostics look terrible here — why do you think that is? Now that we have a full year’s worth of data, for many states the earlier part of that year consisted of a very small number of cases — often zero cases. summary(US_cases_long_demogr_week$cases_count_pos) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0 175.5 2001.0 5695.7 6093.2 94909.0 table(US_cases_long_demogr_week$cases_count_pos == 0) ## ## FALSE TRUE ## 2006 340 About 15% of the data are zeros. This makes the linear model a poor fit for these data. Let’s try a model designed specifically for count data: model_panel2 &lt;- glmmTMB(cases_count_pos ~ 1 + week_of_year + percent_age65over + percent_female + percent_black + (1 | state), family = poisson(link = &quot;log&quot;), data = US_cases_long_demogr_week) # inference summary(model_panel2) ## Family: poisson ( log ) ## Formula: ## cases_count_pos ~ 1 + week_of_year + percent_age65over + percent_female + ## percent_black + (1 | state) ## Data: US_cases_long_demogr_week ## ## AIC BIC logLik deviance df.resid ## 6970899 6970934 -3485444 6970887 2340 ## ## Random effects: ## ## Conditional model: ## Groups Name Variance Std.Dev. ## state (Intercept) 1.189 1.091 ## Number of obs: 2346, groups: state, 51 ## ## Conditional model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -13.37143428 15.97031012 -0.8 0.4024 ## week_of_year 0.06607058 0.00002526 2615.3 &lt;0.0000000000000002 *** ## percent_age65over -0.22649142 0.09699394 -2.3 0.0195 * ## percent_female 0.46168234 0.33838023 1.4 0.1724 ## percent_black -0.00887897 0.02662506 -0.3 0.7388 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # model diagnostics - simulate residuals using the DHARMa package model_panel2_simres &lt;- simulateResiduals(model_panel2) plot(model_panel2_simres, quantreg = TRUE) This looks better. But there are several issues we’re not yet dealing with. Chief among these are: 1) the fact that states have different population levels, but our model is unaware of this, and 2) we have a huge number of zeros in the data, which poisson models are ill-equipped to handle. Let’s fit a more sophisticated model that can account for these issues. We can include an exposure term using the offset() function to get counts per population unit, as well as a separate binomial model to account for the excess of zeros (the zero-inflation component): model_panel3 &lt;- glmmTMB(cases_count_pos ~ 1 + offset(log(pop_count_2019)) + week_of_year + percent_age65over + percent_female + percent_black + (1 | state), family = poisson(link = &quot;log&quot;), ziformula = ~ week_of_year, data = US_cases_long_demogr_week) # inference summary(model_panel3) ## Family: poisson ( log ) ## Formula: ## cases_count_pos ~ 1 + offset(log(pop_count_2019)) + week_of_year + ## percent_age65over + percent_female + percent_black + (1 | state) ## Zero inflation: ~week_of_year ## Data: US_cases_long_demogr_week ## ## AIC BIC logLik deviance df.resid ## 6243994 6244040 -3121989 6243978 2338 ## ## Random effects: ## ## Conditional model: ## Groups Name Variance Std.Dev. ## state (Intercept) 0.2104 0.4587 ## Number of obs: 2346, groups: state, 51 ## ## Conditional model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 7.25747164 6.71704524 1.1 0.2799 ## week_of_year 0.05878122 0.00002682 2191.4 &lt;0.0000000000000002 *** ## percent_age65over -0.04876803 0.04079522 -1.2 0.2319 ## percent_female -0.30602496 0.14232092 -2.2 0.0315 * ## percent_black 0.01955739 0.01119827 1.7 0.0807 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Zero-inflation model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 10.51867 0.88767 11.85 &lt;0.0000000000000002 *** ## week_of_year -1.14758 0.09542 -12.03 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # model diagnostics - simulate residuals using the DHARMa package model_panel3_simres &lt;- simulateResiduals(model_panel3) plot(model_panel3_simres, quantreg = TRUE) This is looking better. Let’s compare our last two models: # use likelihood ratio test to compare models with and without # population exposure and zero-inflation component anova(model_panel2, model_panel3) ## Data: US_cases_long_demogr_week ## Models: ## model_panel2: cases_count_pos ~ 1 + week_of_year + percent_age65over + percent_female + , zi=~0, disp=~1 ## model_panel2: percent_black + (1 | state), zi=~week_of_year, disp=~1 ## model_panel3: cases_count_pos ~ 1 + offset(log(pop_count_2019)) + week_of_year + , zi=~0, disp=~1 ## model_panel3: percent_age65over + percent_female + percent_black + (1 | , zi=~week_of_year, disp=~1 ## model_panel3: state), zi=~0, disp=~1 ## Df AIC BIC logLik deviance Chisq Chi Df ## model_panel2 6 6970899 6970934 -3485444 6970887 ## model_panel3 8 6243994 6244040 -3121989 6243978 726909 2 ## Pr(&gt;Chisq) ## model_panel2 ## model_panel3 &lt; 0.00000000000000022 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Our model_panel3 does a much better job of capturing the idiosyncrasies of our data. We can go further, however. The data may not meet the restrictive assumptions of the poisson model (that the variance is equal to the mean), in which case one option is to fit a negative binomial model that can account for this over- or under-dispersion. We can also include a more flexible random effects structure, to allow each state to have different case count trajectories over time: model_panel4 &lt;- glmmTMB(cases_count_pos ~ 1 + offset(log(pop_count_2019)) + week_of_year + percent_age65over + percent_female + percent_black + (1 + week_of_year | state), family = nbinom2(link = &quot;log&quot;), ziformula = ~ week_of_year, data = US_cases_long_demogr_week) # inference summary(model_panel4) ## Family: nbinom2 ( log ) ## Formula: ## cases_count_pos ~ 1 + offset(log(pop_count_2019)) + week_of_year + ## percent_age65over + percent_female + percent_black + (1 + ## week_of_year | state) ## Zero inflation: ~week_of_year ## Data: US_cases_long_demogr_week ## ## AIC BIC logLik deviance df.resid ## 36194.6 36257.9 -18086.3 36172.6 2335 ## ## Random effects: ## ## Conditional model: ## Groups Name Variance Std.Dev. Corr ## state (Intercept) 1.043021 1.02128 ## week_of_year 0.001259 0.03549 -0.93 ## Number of obs: 2346, groups: state, 51 ## ## Overdispersion parameter for nbinom2 family (): 1.4 ## ## Conditional model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -12.450500 7.002154 -1.778 0.0754 . ## week_of_year 0.076687 0.005287 14.505 &lt;0.0000000000000002 *** ## percent_age65over -0.073991 0.035469 -2.086 0.0370 * ## percent_female 0.079872 0.146119 0.547 0.5846 ## percent_black 0.014524 0.009735 1.492 0.1357 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Zero-inflation model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 10.51630 0.88862 11.83 &lt;0.0000000000000002 *** ## week_of_year -1.14789 0.09557 -12.01 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # model diagnostics - simulate residuals using the DHARMa package model_panel4_simres &lt;- simulateResiduals(model_panel4) plot(model_panel4_simres, quantreg = TRUE) This looks like our best model yet. We can’t compare our previous poisson model and this negative binomial model directly, but we can update our model_panel3 model to use the negative binomial distribution, so that we can determine whether the random slope for time improve the model fit: # update poisson model to use negative binomial family model_panel3 &lt;- update(model_panel3, family = nbinom2) # use likelihood ratio test to compare model with and without random slopes for time anova(model_panel3, model_panel4) ## Data: US_cases_long_demogr_week ## Models: ## model_panel3: cases_count_pos ~ 1 + offset(log(pop_count_2019)) + week_of_year + , zi=~week_of_year, disp=~1 ## model_panel3: percent_age65over + percent_female + percent_black + (1 | , zi=~week_of_year, disp=~1 ## model_panel3: state), zi=~week_of_year, disp=~1 ## model_panel4: cases_count_pos ~ 1 + offset(log(pop_count_2019)) + week_of_year + , zi=~week_of_year, disp=~1 ## model_panel4: percent_age65over + percent_female + percent_black + (1 + , zi=~week_of_year, disp=~1 ## model_panel4: week_of_year | state), zi=~week_of_year, disp=~1 ## Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) ## model_panel3 9 36484 36536 -18233 36466 ## model_panel4 11 36195 36258 -18086 36173 293.69 2 &lt; 0.00000000000000022 ## ## model_panel3 ## model_panel4 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The random slope is a useful addition. Let’s look at some visualizations of the effects from the model: # plot marginal effects for all explanatory variables eff_panel4 &lt;- allEffects(model_panel4) plot(eff_panel4, type = &quot;response&quot;) So far, we’ve only been modeling a linear trend for time. From our visualizations we know that this is unrealistic. How could we incorporate non-linear time elements in the model (e.g., splines, polynomials)? "],
["day-4-data-archiving-r.html", "DAY 4: Data archiving (R)", " DAY 4: Data archiving (R) The data archiving session will probably be a GUI-based overview of using Harvard Dataverse. Though, we could put some demo code out there for how to create and populate a Dataverse entry using the API. "],
["python-setup.html", "Python setup", " Python setup In early January, we’ll create an installation guide for R and RStudio (based on this one: https://iqss.github.io/dss-workshops/Rinstall.html), together with instructions for installing packages, so that all participants can arrive with working Python environments. Here, we create a Conda environment using the bash shell to install all necessary packages: # locally build pyDataverse conda skeleton pypi pyDataverse conda build pyDataverse # create a new virtual environment and activate it conda create -n datafest conda activate datafest # install pyDataverse from local build conda install --use-local pyDataverse # install all other modules after installing pyDataverse conda install requests pandas geopandas numpy matplotlib seaborn descartes bokeh statsmodels # math, functools, and warnings modules are in the Python standard library Now, we can load the necessary packages for the current material: from pyDataverse.api import Api from pyDataverse.models import Dataverse import pandas as pd import numpy as np import requests from functools import reduce import matplotlib.pyplot as plt import math import seaborn as sns # for maps import descartes import geopandas from mpl_toolkits.axes_grid1 import make_axes_locatable from bokeh.plotting import figure, save, show, output_file from bokeh.models import ColumnDataSource, HoverTool, LogColorMapper, ColorBar from bokeh.palettes import Viridis256 as palette # for models import warnings warnings.filterwarnings(&quot;ignore&quot;) import statsmodels.api as sm import scipy.stats as stats from statsmodels.stats.outliers_influence import OLSInfluence as influence import statsmodels.formula.api as smf from statsmodels.genmod.bayes_mixed_glm import PoissonBayesMixedGLM from scipy.stats.distributions import chi2 from statsmodels.genmod.generalized_estimating_equations import GEE from statsmodels.genmod.cov_struct import (Exchangeable, Autoregressive) from statsmodels.genmod.families import (Poisson, NegativeBinomial) "],
["day-1-acquiring-and-cleaning-data-python.html", "DAY 1: Acquiring and cleaning data (Python) Acquiring data from APIs Cleaning data", " DAY 1: Acquiring and cleaning data (Python) Acquiring data from APIs Often, we want to acquire data that is stored online. Online data sources are stored somewhere on a remote server — a remotely located computer that is optimized to process requests for information. Usually, we make requests using a browser, also known as a client, but we can also make requests programmatically. An Application Programming Interface (API) is the part of a remote server that receives requests and sends responses. When we make requests programmatically, the responses an API sends are typically data-based, often in some structured format like JSON or XML. For the project we’ll pursue during DataFest, we’re going to access data stored on the Harvard Dataverse. A Dataverse is open source software for repositing research data. Once data is stored in a Dataverse, it can be accessed programmatically using the Dataverse API. We will use the R package dataverse as an interface for the Dataverse API. Here are three COVID-19 datasets from the Harvard Dataverse: US data on COVID-19 cases and deaths, daily at state-level or county-level: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/HIDLTK US data on COVID-19 cases and deaths, daily at metropolitan-level: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/5B8YM8 World data on COVID-19 cases and deaths, daily at country-level: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/L20LOT As an example of how to use the Dataverse API, we’re going to extract daily data on COVID-19 cases from the U.S. at the state-level (from dataset #1 above). These data span the period from January 21st 2020 until November 29th 2020 for each U.S. state (and the District of Columbia). If you wish, you may choose to use one of the other datasets for your project. We can use the pyDataverse module as an interface for the API: # get the digital object identifier for the Dataverse dataset DOI = &quot;doi:10.7910/DVN/HIDLTK&quot; # establish connection base_url = &#39;https://dataverse.harvard.edu/&#39; api = Api(base_url) print(api.status) # retrieve the contents of the dataset ## OK covid = api.get_dataset(DOI) The covid object is a list of metadata that includes information on all the files stored within this dataset. covid_files_list = covid.json()[&#39;data&#39;][&#39;latestVersion&#39;][&#39;files&#39;] Let’s dig further and display the available files: # view available files for fileObject in covid_files_list: print(&quot;File name is {}; id is {}&quot;.format(fileObject[&quot;dataFile&quot;][&quot;filename&quot;], fileObject[&quot;dataFile&quot;][&quot;id&quot;])) ## File name is COUNTY_MAP.zip; id is 3758784 ## File name is INDEX.txt; id is 3758782 ## File name is METRO_MAP_2018_ESRI.zip; id is 3842202 ## File name is METRO_MAP.zip; id is 3826156 ## File name is Metropolitan_statistical_areas_for_US_counties__Sept_2018.xml; id is 3867748 ## File name is Metropolitan_statistical_areas_for_US_counties__Sept_2018.zip; id is 3867747 ## File name is README.txt; id is 3758780 ## File name is STATE_MAP.zip; id is 3758781 ## File name is us_county_confirmed_cases.tab; id is 4202274 ## File name is us_county_deaths_cases.tab; id is 4202273 ## File name is us_state_confirmed_case.tab; id is 4201597 ## File name is us_state_deaths_case.tab; id is 4201596 For our example project, we’re going to use the data on cumulative COVID-19 cases at the state-level contained in the us_state_confirmed_case.tab file. # get data file for COVID-19 cases US_states_cases_file = api.get_datafile(&quot;4201597&quot;) To convert the data into a more user friendly data frame, we have to jump through a few hoops: # convert in_text = US_states_cases_file.content tmp = &quot;data_py/US_states_cases.tab&quot; f = open(tmp, &quot;wb&quot;) f.write(in_text) ## 91400 f.close() US_states_cases = pd.read_csv(tmp, sep=&#39;\\t&#39;) We can now inspect the data: # inspect the data print(US_states_cases.head(10)) # 50 states plus DC by 314 days ## fips NAME ... 2020-11-28 2020-11-29 ## 0 1 Alabama ... 244993 247229 ## 1 2 Alaska ... 31279 31896 ## 2 4 Arizona ... 322774 325995 ## 3 5 Arkansas ... 155026 156247 ## 4 6 California ... 1206278 1219496 ## 5 8 Colorado ... 226567 230057 ## 6 9 Connecticut ... 112581 112581 ## 7 10 Delaware ... 34670 35251 ## 8 11 District of Columbia ... 21308 21448 ## 9 12 Florida ... 985289 992652 ## ## [10 rows x 326 columns] Cleaning data COVID-19 cases data The COVID-19 cases data are in wide format, with individual columns for each day’s case counts. To visualize and analyze the data, it will be much easier to reshape the data so that it is organized in long format, with a single column for case counts and another column indicating the date those counts are associated with. In addition, it will be useful to derive some time-related variables (e.g., day of year, week of year) from the dates. Finally, we should transform our cumulative case counts into regular counts and create some rate variables by normalizing by population count. # select columns of interest US_states_cases_filtered = US_states_cases.filter(regex=&quot;^\\\\d&quot;, axis=1) US_states_cases_selected = US_states_cases.loc[:, [&#39;fips&#39;, &#39;NAME&#39;, &#39;POP10&#39;]] US_states_cases_selected = US_states_cases_selected.assign(**US_states_cases_filtered) # rename some columns US_states_cases_selected = US_states_cases_selected.rename(columns={&#39;fips&#39;:&#39;GEOID&#39;, &#39;NAME&#39;:&#39;state&#39;, &#39;POP10&#39;:&#39;pop_count_2010&#39;}) # reshape to long format for dates US_states_cases_selected = pd.melt(US_states_cases_selected, id_vars=[&quot;GEOID&quot;, &quot;state&quot;, &quot;pop_count_2010&quot;], var_name=&#39;date&#39;, value_name=&quot;cases_cum&quot;) US_states_cases_selected = US_states_cases_selected.sort_values([&#39;GEOID&#39;, &#39;date&#39;]).reset_index(drop=True) # create new derived time variables from dates US_states_cases_selected[&quot;day_of_year&quot;] = pd.to_datetime(US_states_cases_selected.date).dt.dayofyear US_states_cases_selected[&quot;week_of_year&quot;] = pd.to_datetime(US_states_cases_selected.date).dt.isocalendar().week US_states_cases_selected[&quot;month&quot;] = pd.to_datetime(US_states_cases_selected.date).dt.month # create cases counts US_states_cases_selected[&quot;cases_count&quot;] = US_states_cases_selected.groupby(&#39;state&#39;).cases_cum.apply(lambda x: x - x.shift(1)).fillna(0) # tidy-up negative counts US_states_cases_selected[&quot;cases_count_pos&quot;] = np.where(US_states_cases_selected[&quot;cases_count&quot;] &lt; 0, 0, US_states_cases_selected[&quot;cases_count&quot;]) # create cases rates US_states_cases_selected[&quot;cases_rate_100K&quot;] = (US_states_cases_selected[&quot;cases_count_pos&quot;] / US_states_cases_selected[&quot;pop_count_2010&quot;]) * 1e5 US_states_cases_selected[&quot;cases_cum_rate_100K&quot;] = (US_states_cases_selected[&quot;cases_cum&quot;] / US_states_cases_selected[&quot;pop_count_2010&quot;]) * 1e5 US_states_cases_selected.to_csv(&quot;data_py/US_states_cases_selected.csv&quot;) # 16014 observations (50 states + 1 DC * 314 days) Aggregate data The cleaned data object US_states_cases_selected has 16,014 observations (50 states + 1 DC * 314 days). For visualization, this should be fine in most cases. When we come to build models for these data, they may take a long time to run. If we’re mainly interested in longer term trends, we can probably get a good approximation by aggregating the data to the weekly level for modeling: # aggregate to weekly level (for later modeling) aggs_by_col = {&#39;pop_count_2010&#39;: lambda x: np.mean(x), &#39;cases_cum&#39;: &#39;sum&#39;, &#39;cases_cum_rate_100K&#39;: &#39;sum&#39;, &#39;cases_count_pos&#39;: &#39;sum&#39;, &#39;cases_rate_100K&#39;: &#39;sum&#39;} US_states_cases_week = US_states_cases_selected.groupby([&#39;GEOID&#39;, &#39;state&#39;, &#39;week_of_year&#39;], as_index=False).agg(aggs_by_col) print(US_states_cases_week.head(20)) ## GEOID state ... cases_count_pos cases_rate_100K ## 0 1 Alabama ... 0.0 0.000000 ## 1 1 Alabama ... 0.0 0.000000 ## 2 1 Alabama ... 0.0 0.000000 ## 3 1 Alabama ... 0.0 0.000000 ## 4 1 Alabama ... 0.0 0.000000 ## 5 1 Alabama ... 0.0 0.000000 ## 6 1 Alabama ... 0.0 0.000000 ## 7 1 Alabama ... 23.0 0.481198 ## 8 1 Alabama ... 134.0 2.803502 ## 9 1 Alabama ... 673.0 14.080276 ## 10 1 Alabama ... 1010.0 21.130874 ## 11 1 Alabama ... 1743.0 36.466449 ## 12 1 Alabama ... 1320.0 27.616588 ## 13 1 Alabama ... 1518.0 31.759076 ## 14 1 Alabama ... 1467.0 30.692072 ## 15 1 Alabama ... 2001.0 41.864237 ## 16 1 Alabama ... 1882.0 39.374560 ## 17 1 Alabama ... 2707.0 56.634927 ## 18 1 Alabama ... 3474.0 72.681838 ## 19 1 Alabama ... 2548.0 53.308384 ## ## [20 rows x 8 columns] US_states_cases_week.to_csv(&quot;data_py/US_states_cases_week.csv&quot;) "],
["optional-u-s-census-data-python.html", "OPTIONAL: U.S. Census data (Python) U.S. Census Bureau API Clean U.S. Census data Combine Census and COVID-19 data Aggregate to weekly-level", " OPTIONAL: U.S. Census data (Python) This section is optional. It provides an example of how to acquire potentially interesting predictors of COVID-19 cases from the U.S. Census Bureau. The COVID-19 dataset we accessed above provides daily COVID-19 case counts for each U.S State, together with population counts from the 2010 Decennial Census. This should be enough information to produce some interesting visualizations. For modeling, however, we really only have one useful predictor in the dataset — time. This section describes some options for acquiring other potentially interesting predictors of COVID-19 cases. U.S. Census Bureau API We may want to use additional demographic information in our visualizations and analysis of the COVID-19 cases. An obvious place to source this information is from the U.S. Census Bureau. There are three U.S. Census Bureau data sources, each with their own API: Decennial Census: survey of every household in the U.S. every 10 years — used to calculate population of U.S. geographic areas. American Community Survey: yearly representative sample of 3.5 million households — used to calculate population estimates of U.S. geographic areas. Population Estimates: yearly population estimates of U.S. geographic areas. The COVID-19 data from Dataverse already contains population values from the 2010 decennial census. But, using the Census Bureau’s Population Estimates API, we can get updated population data for 2019 as well as population data stratified by age groups, race, and sex. We’re going to use the requests package as an interface to the Census Bureau API. The first step is to sign-up for an API key: http://api.census.gov/data/key_signup.html. Then give the key a name. # store API key API_key = &quot;your-API-key-here&quot; Next, we can use the .get() method to access the Population Estimates API and extract variables of interest into a pandas data frame: # access the Population Estimates API and extract variables of interest # provides overall population estimates and population densities pop_url = f&#39;https://api.census.gov/data/2019/pep/population?get=NAME,POP,DENSITY&amp;for=state:*&amp;key={API_key}&#39; response = requests.get(pop_url) pop_data = response.json() pop_df = pd.DataFrame(pop_data[1:], columns=pop_data[0]).rename(columns={&#39;NAME&#39;:&#39;state&#39;, &#39;state&#39;:&#39;GEOID&#39;}) print(pop_df.head(20)) ## state POP DENSITY GEOID ## 0 Mississippi 2976149 63.42279731700000 28 ## 1 Missouri 6137428 89.27757816400000 29 ## 2 Montana 1068778 7.34314521650000 30 ## 3 Nebraska 1934408 25.18219543000000 31 ## 4 Nevada 3080156 28.03700636500000 32 ## 5 New Hampshire 1359711 151.86532690000000 33 ## 6 New Jersey 8882190 1207.67221710000000 34 ## 7 New Mexico 2096829 17.28456209900000 35 ## 8 New York 19453561 412.81842864000000 36 ## 9 North Carolina 10488084 215.71420423000000 37 ## 10 North Dakota 762062 11.04521346200000 38 ## 11 Ohio 11689100 286.08552119000000 39 ## 12 Oklahoma 3956971 57.68520257700000 40 ## 13 Oregon 4217737 43.94025129200000 41 ## 14 Pennsylvania 12801989 286.13105806000000 42 ## 15 Rhode Island 1059361 1024.62672880000000 44 ## 16 South Carolina 5148714 171.26007636000000 45 ## 17 South Dakota 884659 11.66947080600000 46 ## 18 Tennessee 6829174 165.60404231000000 47 ## 19 Texas 28995881 110.98346670000000 48 Get population estimates by age group: age_url = f&#39;https://api.census.gov/data/2019/pep/charagegroups?get=NAME,AGEGROUP,POP&amp;for=state:*&amp;key={API_key}&#39; response = requests.get(age_url) age_data = response.json() age_df = pd.DataFrame(age_data[1:], columns=age_data[0]).rename(columns={&#39;NAME&#39;:&#39;state&#39;, &#39;state&#39;:&#39;GEOID&#39;}) print(age_df.head(20)) ## state AGEGROUP POP GEOID ## 0 Mississippi 0 2976149 28 ## 1 Mississippi 1 183478 28 ## 2 Mississippi 2 189377 28 ## 3 Mississippi 16 87629 28 ## 4 Mississippi 17 57811 28 ## 5 Mississippi 18 52298 28 ## 6 Mississippi 21 159872 28 ## 7 Mississippi 22 1790873 28 ## 8 Mississippi 23 283421 28 ## 9 Mississippi 24 759501 28 ## 10 Mississippi 25 747951 28 ## 11 Mississippi 26 486693 28 ## 12 Mississippi 27 52298 28 ## 13 Mississippi 28 2356806 28 ## 14 Mississippi 29 2277566 28 ## 15 Mississippi 30 1162368 28 ## 16 Mississippi 31 38 28 ## 17 Mississippi 3 206282 28 ## 18 Mississippi 4 201350 28 ## 19 Mississippi 5 201517 28 Get population estimates by sex: sex_url = f&#39;https://api.census.gov/data/2019/pep/charagegroups?get=NAME,SEX,POP&amp;for=state:*&amp;key={API_key}&#39; response = requests.get(sex_url) sex_data = response.json() sex_df = pd.DataFrame(sex_data[1:], columns=sex_data[0]).rename(columns={&#39;NAME&#39;:&#39;state&#39;, &#39;state&#39;:&#39;GEOID&#39;}) print(sex_df.head(20)) ## state SEX POP GEOID ## 0 Mississippi 0 2976149 28 ## 1 Mississippi 1 1442292 28 ## 2 Mississippi 2 1533857 28 ## 3 Missouri 0 6137428 29 ## 4 Missouri 1 3012662 29 ## 5 Missouri 2 3124766 29 ## 6 Montana 0 1068778 30 ## 7 Montana 1 538066 30 ## 8 Montana 2 530712 30 ## 9 Nebraska 0 1934408 31 ## 10 Nevada 0 3080156 32 ## 11 New Hampshire 0 1359711 33 ## 12 New Jersey 0 8882190 34 ## 13 New Mexico 0 2096829 35 ## 14 New Mexico 1 1037432 35 ## 15 New York 0 19453561 36 ## 16 New York 1 9447846 36 ## 17 New York 2 10005715 36 ## 18 North Carolina 0 10488084 37 ## 19 North Dakota 0 762062 38 Get population estimates by race: race_url = f&#39;https://api.census.gov/data/2019/pep/charagegroups?get=NAME,RACE,POP&amp;for=state:*&amp;key={API_key}&#39; response = requests.get(race_url) race_data = response.json() race_df = pd.DataFrame(race_data[1:], columns=race_data[0]).rename(columns={&#39;NAME&#39;:&#39;state&#39;, &#39;state&#39;:&#39;GEOID&#39;}) print(race_df.head(20)) ## state RACE POP GEOID ## 0 Mississippi 0 2976149 28 ## 1 Mississippi 1 1758081 28 ## 2 Mississippi 2 1124559 28 ## 3 Mississippi 3 18705 28 ## 4 Mississippi 4 33032 28 ## 5 Mississippi 5 1806 28 ## 6 Mississippi 6 39966 28 ## 7 Mississippi 7 1792535 28 ## 8 Mississippi 8 1148593 28 ## 9 Mississippi 9 31669 28 ## 10 Mississippi 10 42550 28 ## 11 Mississippi 11 3821 28 ## 12 Missouri 0 6137428 29 ## 13 Missouri 1 5086188 29 ## 14 Missouri 2 725579 29 ## 15 Missouri 3 35839 29 ## 16 Missouri 4 133312 29 ## 17 Missouri 5 9857 29 ## 18 Missouri 6 146653 29 ## 19 Missouri 7 5222409 29 Clean U.S. Census data The Census data we extracted contain population estimates for multiple categories of age, race, and sex. It will be useful to simplify these data by creating some derived variables that may be of interest when visualizing and analyzing the data. For example, for each state, we may want to calculate: Overall population count and density Proportion of people that are 65 years and older Proportion of people that are female (or male) Proportion of people that are black (or white, or other race) Overall population estimates: # order by GEOID (same as state FIPS code) pop_df[[&quot;GEOID&quot;, &quot;POP&quot;]] = pop_df[[&quot;GEOID&quot;, &quot;POP&quot;]].apply(pd.to_numeric) pop_wide = pop_df.sort_values([&#39;GEOID&#39;]).reset_index(drop=True) # exclude Puerto Rico and rename some variables # data are already in wide format - no need to reshape pop_wide = pop_wide[pop_wide.state != &quot;Puerto Rico&quot;].rename(columns={&#39;POP&#39;:&#39;pop_count_2019&#39;, &#39;DENSITY&#39;:&#39;pop_density_2019&#39;}) print(pop_wide.head(20)) ## state pop_count_2019 pop_density_2019 GEOID ## 0 Alabama 4903185 96.81165243500000 1 ## 1 Alaska 731545 1.28112659200000 2 ## 2 Arizona 7278717 64.04325175200000 4 ## 3 Arkansas 3017804 57.99283630500000 5 ## 4 California 39512223 253.52068818000000 6 ## 5 Colorado 5758736 55.56614283700000 8 ## 6 Connecticut 3565287 736.22104642000000 9 ## 7 Delaware 973764 499.74775877000000 10 ## 8 District of Columbia 705749 11543.99993900000000 11 ## 9 Florida 21477737 400.34642766000000 12 ## 10 Georgia 10617423 183.95873142000000 13 ## 11 Hawaii 1415872 220.45750094000000 15 ## 12 Idaho 1787065 21.62335488800000 16 ## 13 Illinois 12671821 228.26467808000000 17 ## 14 Indiana 6732219 187.91203111000000 18 ## 15 Iowa 3155070 56.48808963000000 19 ## 16 Kansas 2913314 35.63317347300000 20 ## 17 Kentucky 4467673 113.13031412000000 21 ## 18 Louisiana 4648794 107.59979292000000 22 ## 19 Maine 1344212 43.57985940900000 23 Population estimates by age group: # convert some variables to numeric age_df[[&quot;GEOID&quot;, &quot;AGEGROUP&quot;, &quot;POP&quot;]] = age_df[[&quot;GEOID&quot;, &quot;AGEGROUP&quot;, &quot;POP&quot;]].apply(pd.to_numeric) # order by GEOID (same as state FIPS code) age_df = age_df.sort_values([&#39;GEOID&#39;, &#39;AGEGROUP&#39;]).reset_index(drop=True) # reshape the age groups to wide format age_wide = age_df.pivot_table(index=[&quot;GEOID&quot;, &quot;state&quot;], columns=&#39;AGEGROUP&#39;, values=&quot;POP&quot;).reset_index() # create variable for percent of people that are 65 years and older age_wide[&quot;percent_age65over&quot;] = (age_wide[26] / age_wide[0]) * 100 # select columns of interest age_wide = age_wide.loc[:, [&#39;GEOID&#39;, &#39;state&#39;, &#39;percent_age65over&#39;]] # exclude Puerto Rico age_wide = age_wide[age_wide.state != &quot;Puerto Rico&quot;] print(age_wide.head(20)) ## AGEGROUP GEOID state percent_age65over ## 0 1 Alabama 17.332346 ## 1 2 Alaska 12.519804 ## 2 4 Arizona 17.978897 ## 3 5 Arkansas 17.359709 ## 4 6 California 14.775466 ## 5 8 Colorado 14.628418 ## 6 9 Connecticut 17.677230 ## 7 10 Delaware 19.399567 ## 8 11 District of Columbia 12.375930 ## 9 12 Florida 20.939529 ## 10 13 Georgia 14.287403 ## 11 15 Hawaii 18.959906 ## 12 16 Idaho 16.265217 ## 13 17 Illinois 16.124218 ## 14 18 Indiana 16.127565 ## 15 19 Iowa 17.525887 ## 16 20 Kansas 16.321172 ## 17 21 Kentucky 16.799887 ## 18 22 Louisiana 15.940091 ## 19 23 Maine 21.221727 Population estimates by sex: # convert some variables to numeric sex_df[[&quot;GEOID&quot;, &quot;SEX&quot;, &quot;POP&quot;]] = sex_df[[&quot;GEOID&quot;, &quot;SEX&quot;, &quot;POP&quot;]].apply(pd.to_numeric) # order by GEOID (same as state FIPS code) sex_df = sex_df.sort_values([&#39;GEOID&#39;, &#39;SEX&#39;]).reset_index(drop=True) # reshape the sex groups to wide format sex_wide = sex_df.pivot_table(index=[&quot;GEOID&quot;, &quot;state&quot;], columns=&#39;SEX&#39;, values=&quot;POP&quot;).reset_index() # create variable for percent of people that are female sex_wide[&quot;percent_female&quot;] = (sex_wide[2] / sex_wide[0]) * 100 # select columns of interest sex_wide = sex_wide.loc[:, [&#39;GEOID&#39;, &#39;state&#39;, &#39;percent_female&#39;]] # exclude Puerto Rico sex_wide = sex_wide[sex_wide.state != &quot;Puerto Rico&quot;] print(sex_wide.head(20)) ## SEX GEOID state percent_female ## 0 1 Alabama 51.673922 ## 1 2 Alaska 47.861307 ## 2 4 Arizona 50.303096 ## 3 5 Arkansas 50.904167 ## 4 6 California 50.281577 ## 5 8 Colorado 49.618319 ## 6 9 Connecticut 51.212427 ## 7 10 Delaware 51.652248 ## 8 11 District of Columbia 52.573649 ## 9 12 Florida 51.125875 ## 10 13 Georgia 51.396643 ## 11 15 Hawaii 49.992655 ## 12 16 Idaho 49.865953 ## 13 17 Illinois 50.862958 ## 14 18 Indiana 50.679560 ## 15 19 Iowa 50.204750 ## 16 20 Kansas 50.170253 ## 17 21 Kentucky 50.734331 ## 18 22 Louisiana 51.233589 ## 19 23 Maine 51.041056 Population estimates by race: # convert some variables to numeric race_df[[&quot;GEOID&quot;, &quot;RACE&quot;, &quot;POP&quot;]] = race_df[[&quot;GEOID&quot;, &quot;RACE&quot;, &quot;POP&quot;]].apply(pd.to_numeric) # order by GEOID (same as state FIPS code) race_df = race_df.sort_values([&#39;GEOID&#39;, &#39;RACE&#39;]).reset_index(drop=True) # reshape the race categories to wide format race_wide = race_df.pivot_table(index=[&quot;GEOID&quot;, &quot;state&quot;], columns=&#39;RACE&#39;, values=&quot;POP&quot;).reset_index() # create variables for percentages of people that are black and white race_wide[&quot;percent_white&quot;] = (race_wide[1] / race_wide[0]) * 100 race_wide[&quot;percent_black&quot;] = (race_wide[2] / race_wide[0]) * 100 # select columns of interest race_wide = race_wide.loc[:, [&#39;GEOID&#39;, &#39;state&#39;, &#39;percent_white&#39;, &#39;percent_black&#39;]] # exclude Puerto Rico race_wide = race_wide[race_wide.state != &quot;Puerto Rico&quot;] print(race_wide.head(20)) ## RACE GEOID state percent_white percent_black ## 0 1 Alabama 69.126415 26.784447 ## 1 2 Alaska 65.271173 3.705582 ## 2 4 Arizona 82.616785 5.179443 ## 3 5 Arkansas 79.039527 15.675239 ## 4 6 California 71.939104 6.460677 ## 5 8 Colorado 86.939426 4.592657 ## 6 9 Connecticut 79.700204 12.189341 ## 7 10 Delaware 69.223652 23.162080 ## 8 11 District of Columbia 45.990997 45.977253 ## 9 12 Florida 77.277192 16.917588 ## 10 13 Georgia 60.198270 32.570493 ## 11 15 Hawaii 25.519044 2.186497 ## 12 16 Idaho 93.022078 0.914684 ## 13 17 Illinois 76.760199 14.619177 ## 14 18 Indiana 84.809956 9.946141 ## 15 19 Iowa 90.618592 4.060290 ## 16 20 Kansas 86.288193 6.134766 ## 17 21 Kentucky 87.515872 8.471502 ## 18 22 Louisiana 62.787101 32.798463 ## 19 23 Maine 94.429004 1.688052 We can now merge all the cleaned Census data into one object called demographics: data_frames = [pop_wide, age_wide, sex_wide, race_wide] demographics = reduce(lambda left,right: pd.merge(left,right,on=[&#39;GEOID&#39;, &#39;state&#39;], how=&#39;left&#39;), data_frames) print(demographics.head(20)) ## state ... percent_black ## 0 Alabama ... 26.784447 ## 1 Alaska ... 3.705582 ## 2 Arizona ... 5.179443 ## 3 Arkansas ... 15.675239 ## 4 California ... 6.460677 ## 5 Colorado ... 4.592657 ## 6 Connecticut ... 12.189341 ## 7 Delaware ... 23.162080 ## 8 District of Columbia ... 45.977253 ## 9 Florida ... 16.917588 ## 10 Georgia ... 32.570493 ## 11 Hawaii ... 2.186497 ## 12 Idaho ... 0.914684 ## 13 Illinois ... 14.619177 ## 14 Indiana ... 9.946141 ## 15 Iowa ... 4.060290 ## 16 Kansas ... 6.134766 ## 17 Kentucky ... 8.471502 ## 18 Louisiana ... 32.798463 ## 19 Maine ... 1.688052 ## ## [20 rows x 8 columns] Combine Census and COVID-19 data Merge the COVID-19 cases data with Census demographic data: # merge COVID-19 cases with demographics data_frames = [US_states_cases_selected, demographics] US_cases_long_demogr = reduce(lambda left,right: pd.merge(left,right,on=[&#39;GEOID&#39;, &#39;state&#39;], how=&#39;left&#39;), data_frames) # update the case rate variables to use population estimates from 2019 US_cases_long_demogr[&quot;cases_cum_rate_100K&quot;] = (US_cases_long_demogr[&quot;cases_cum&quot;] / US_cases_long_demogr[&quot;pop_count_2019&quot;]) * 1e5 US_cases_long_demogr[&quot;cases_rate_100K&quot;] = (US_cases_long_demogr[&quot;cases_count_pos&quot;] / US_cases_long_demogr[&quot;pop_count_2019&quot;]) * 1e5 print(US_cases_long_demogr.head(20)) ## GEOID state ... percent_white percent_black ## 0 1 Alabama ... 69.126415 26.784447 ## 1 1 Alabama ... 69.126415 26.784447 ## 2 1 Alabama ... 69.126415 26.784447 ## 3 1 Alabama ... 69.126415 26.784447 ## 4 1 Alabama ... 69.126415 26.784447 ## 5 1 Alabama ... 69.126415 26.784447 ## 6 1 Alabama ... 69.126415 26.784447 ## 7 1 Alabama ... 69.126415 26.784447 ## 8 1 Alabama ... 69.126415 26.784447 ## 9 1 Alabama ... 69.126415 26.784447 ## 10 1 Alabama ... 69.126415 26.784447 ## 11 1 Alabama ... 69.126415 26.784447 ## 12 1 Alabama ... 69.126415 26.784447 ## 13 1 Alabama ... 69.126415 26.784447 ## 14 1 Alabama ... 69.126415 26.784447 ## 15 1 Alabama ... 69.126415 26.784447 ## 16 1 Alabama ... 69.126415 26.784447 ## 17 1 Alabama ... 69.126415 26.784447 ## 18 1 Alabama ... 69.126415 26.784447 ## 19 1 Alabama ... 69.126415 26.784447 ## ## [20 rows x 18 columns] Aggregate to weekly-level Once again, for the purposes of modeling, it may be useful to aggregate to the weekly-level: # COVID-19 data and demographic data aggs_by_col = {&#39;pop_count_2019&#39;: lambda x: np.mean(x), &#39;percent_age65over&#39;: lambda x: np.mean(x), &#39;percent_female&#39;: lambda x: np.mean(x), \\ &#39;percent_white&#39;: lambda x: np.mean(x), &#39;percent_black&#39;: lambda x: np.mean(x),&#39;cases_cum&#39;: &#39;sum&#39;, &#39;cases_cum_rate_100K&#39;: &#39;sum&#39;, \\ &#39;cases_count_pos&#39;: &#39;sum&#39;, &#39;cases_rate_100K&#39;: &#39;sum&#39;} US_cases_long_demogr_week = US_cases_long_demogr.groupby([&#39;state&#39;, &#39;week_of_year&#39;], as_index=False).agg(aggs_by_col) print(US_cases_long_demogr_week.head(20)) ## state week_of_year ... cases_count_pos cases_rate_100K ## 0 Alabama 4 ... 0.0 0.000000 ## 1 Alabama 5 ... 0.0 0.000000 ## 2 Alabama 6 ... 0.0 0.000000 ## 3 Alabama 7 ... 0.0 0.000000 ## 4 Alabama 8 ... 0.0 0.000000 ## 5 Alabama 9 ... 0.0 0.000000 ## 6 Alabama 10 ... 0.0 0.000000 ## 7 Alabama 11 ... 23.0 0.469083 ## 8 Alabama 12 ... 134.0 2.732917 ## 9 Alabama 13 ... 673.0 13.725772 ## 10 Alabama 14 ... 1010.0 20.598856 ## 11 Alabama 15 ... 1743.0 35.548322 ## 12 Alabama 16 ... 1320.0 26.921277 ## 13 Alabama 17 ... 1518.0 30.959468 ## 14 Alabama 18 ... 1467.0 29.919328 ## 15 Alabama 19 ... 2001.0 40.810208 ## 16 Alabama 20 ... 1882.0 38.383214 ## 17 Alabama 21 ... 2707.0 55.209012 ## 18 Alabama 22 ... 3474.0 70.851905 ## 19 Alabama 23 ... 2548.0 51.966222 ## ## [20 rows x 11 columns] Let’s store the data frame in a .csv file so that we can easily access it later: # store the data frame in a .csv file US_cases_long_demogr_week.to_csv(&quot;data_py/US_cases_long_demogr_week.csv&quot;) "],
["day-2-data-visualization-python.html", "DAY 2: Data visualization (Python) Non-spatial graphs Static Maps Interactive Maps", " DAY 2: Data visualization (Python) The COVID-19 cases data we have are inherently temporal and spatial. Let’s explore the space and time dimensions of the case data through visualization. Non-spatial graphs Let’s load the daily cases data we cleaned yesterday: # get dataframe US_states_cases_selected = pd.read_csv(&quot;data_py/US_states_cases_selected.csv&quot;) We can easily create a wide range of non-spatial graphs using the seaborn module. We can start with a very simple line graph of the COVID-19 cases rates over time: # line graph of covid cases rates sns.lineplot(&#39;date&#39;, &#39;cases_rate_100K&#39;, data = US_states_cases_selected, hue=&#39;state&#39;, legend=False) plt.xlabel(&quot;$date$&quot;, fontsize=10) plt.ylabel(&quot;$cases_rate_100K$&quot;, fontsize=10, rotation=90) plt.show() This gives us an overall sense that the rate of cases has increased over time and has become particularly prevalent in the fall of 2020. But, because the lines for each state are not discernible, we can’t see if some states have a different trajectory of case rates than other states. A better solution is to use faceting to produce mini-plots for each state. Let’s create a new line graph of COVID-19 cases rates over time, this time with a separate mini-plot for each state: # line graphs of covid cases rates for each state state_group = US_states_cases_selected[&quot;state&quot;].value_counts().index.tolist() state_group.sort() num_state_group = len(state_group) if num_state_group % 8 == 0: rows = math.ceil(num_state_group/8) + 1 else: rows = math.ceil(num_state_group/8) fig, axs = plt.subplots(rows, 8, squeeze=False, sharex=True, figsize=(10, 10)) fig.tight_layout() fig.text(0.5, 0.02, &#39;date&#39;, ha=&#39;center&#39;) fig.text(0.005, 0.5, &#39;cases_rate_100K&#39;, va=&#39;center&#39;, rotation=&#39;vertical&#39;) for i in range(len(state_group)): quodient = i//8 remainder = i % 8 focal_sample = state_group[i] temp = US_states_cases_selected.loc[(US_states_cases_selected.state == focal_sample), :] axs[quodient,remainder].plot(temp[&#39;date&#39;], temp[&#39;cases_rate_100K&#39;]) axs[quodient,remainder].set_xticks(temp[&#39;date&#39;], minor=True) axs[quodient,remainder].set_title(label=focal_sample, loc=&#39;center&#39;, fontsize=&quot;x-small&quot;) if remainder != 5: for j in range(remainder + 1, 8): fig.delaxes(axs[quodient, j]) plt.show() We can try the same strategy for cumulative COVID-19 case rates over time. First, in a graph that jumbles together all the states: # line graph of cumulative covid cases rates sns.lineplot(&#39;date&#39;, &#39;cases_cum_rate_100K&#39;, data = US_states_cases_selected, hue=&#39;state&#39;, legend=False) plt.xlabel(&quot;$date$&quot;, fontsize=10) plt.ylabel(&quot;$cases_cum_rate_100K$&quot;, fontsize=10, rotation=90) plt.show() Again, we get a sense of the overall trend here, but we can get a much better picture of state-level differences by faceting. So, let’s create a new line graph of COVID-19 cumulative cases rates over time, this time with a separate mini-plot for each state: # line graphs of cumulative covid cases rates for each state state_group_cum = US_states_cases_selected[&quot;state&quot;].value_counts().index.tolist() state_group_cum.sort() num_state_group_cum = len(state_group_cum) if num_state_group_cum % 8 == 0: rows = math.ceil(num_state_group_cum/8) + 1 else: rows = math.ceil(num_state_group_cum/8) fig, axs = plt.subplots(rows, 8, squeeze=False, sharex=True, figsize=(10, 10)) fig.tight_layout() fig.text(0.5, 0.02, &#39;date&#39;, ha=&#39;center&#39;) fig.text(0.005, 0.5, &#39;cases_cum_rate_100K&#39;, va=&#39;center&#39;, rotation=&#39;vertical&#39;) for i in range(len(state_group_cum)): quodient = i//8 remainder = i % 8 focal_sample = state_group_cum[i] temp = US_states_cases_selected.loc[(US_states_cases_selected.state == focal_sample), :] axs[quodient,remainder].plot(temp[&#39;date&#39;], temp[&#39;cases_cum_rate_100K&#39;]) axs[quodient,remainder].set_xticks(temp[&#39;date&#39;], minor=True) axs[quodient,remainder].set_title(label=focal_sample, loc=&#39;center&#39;, fontsize=&quot;x-small&quot;) if remainder != 5: for j in range(remainder + 1, 8): fig.delaxes(axs[quodient, j]) plt.show() Static Maps A great way to visualize spatial relationships in data is to superimpose variables onto a map. For some datasets, this could involve superimposing points or lines. For our state-level data, this will involve coloring state polygons in proportion to a variable of interest that represents an aggregate summary of a geographic characteristic within each state. Such a graph is often referred to as a choropleth map. To create a choropleth map we first need to acquire shapefiles that contain spatial data about U.S. state-level geographies. We can use the Census Tiger shape files for census geographies. We want state-level geographies, which we can download by putting the following URL into our browser: https://www2.census.gov/geo/tiger/TIGER2019/STATE/tl_2019_us_state.zip. We then need to unzip the directory to gain access to the shapefile named tl_2019_us_state.shp. Alternatively, we can do this programmatically using the UNIX bash shell, either using wget or, in this example, curl: # make a new directory for the shapefiles mkdir shapefiles # move to the new directory cd shapefiles # download the zipped shapefiles curl -sS https://www2.census.gov/geo/tiger/TIGER2019/STATE/tl_2019_us_state.zip &gt; file.zip # unzip the shapefiles unzip file.zip # remove the zipped directory rm file.zip Let’s read the shapefile and clean it: # read the shapefile us_state_geo = geopandas.read_file(&#39;shapefiles/tl_2019_us_state.shp&#39;) # rename `NAME` variable to `state` us_state_geo = us_state_geo.rename(columns={&#39;NAME&#39;:&#39;state&#39;}) # clean us_state_geo[[&quot;GEOID&quot;]] = us_state_geo[[&quot;GEOID&quot;]].apply(pd.to_numeric) us_state_geo = us_state_geo[(us_state_geo.GEOID &lt; 60) &amp; (us_state_geo.state != &quot;Alaska&quot;) &amp; (us_state_geo.state != &quot;Hawaii&quot;)] print(us_state_geo.head(20)) # us_state_geo = us_state_geo.to_crs(&quot;EPSG:3395&quot;) ## REGION ... geometry ## 0 3 ... POLYGON ((-81.74725 39.09538, -81.74635 39.096... ## 1 3 ... MULTIPOLYGON (((-86.38865 30.99418, -86.38385 ... ## 2 2 ... POLYGON ((-91.18529 40.63780, -91.17510 40.643... ## 3 2 ... POLYGON ((-96.78438 46.63050, -96.78434 46.630... ## 4 3 ... POLYGON ((-77.45881 39.22027, -77.45866 39.220... ## 5 1 ... MULTIPOLYGON (((-71.78970 41.72520, -71.78971 ... ## 6 4 ... POLYGON ((-116.89971 44.84061, -116.89967 44.8... ## 7 1 ... POLYGON ((-72.32990 43.60021, -72.32984 43.600... ## 8 3 ... POLYGON ((-82.41674 36.07283, -82.41660 36.073... ## 9 1 ... POLYGON ((-73.31328 44.26413, -73.31274 44.265... ## 10 1 ... POLYGON ((-73.51808 41.66672, -73.51807 41.666... ## 11 3 ... POLYGON ((-75.76007 39.29682, -75.76010 39.297... ## 12 4 ... POLYGON ((-106.00632 36.99527, -106.00531 36.9... ## 13 4 ... MULTIPOLYGON (((-124.13656 41.46445, -124.1378... ## 14 1 ... POLYGON ((-75.18960 40.59178, -75.18977 40.592... ## 15 2 ... POLYGON ((-92.88707 45.64415, -92.88671 45.644... ## 16 4 ... POLYGON ((-124.06545 45.78305, -124.06206 45.7... ## 17 2 ... POLYGON ((-104.05264 42.00172, -104.05263 42.0... ## 18 1 ... POLYGON ((-80.51935 41.84956, -80.51938 41.850... ## 19 4 ... POLYGON ((-123.24792 48.28456, -123.24751 48.2... ## ## [20 rows x 15 columns] Now let’s read in the weekly cases data we cleaned yesterday: # use previously cleaned weekly dataset US_states_cases_week = pd.read_csv(&quot;data_py/US_states_cases_week.csv&quot;) We can now merge the spatial data with our weekly COVID-19 cases data, keeping only the contiguous 48 states (plus D.C.): # merge weekly COVID-19 cases with spatial data US_states_cases_week[[&quot;GEOID&quot;]] = US_states_cases_week[[&quot;GEOID&quot;]].apply(pd.to_numeric) data_frames = [us_state_geo, US_states_cases_week] US_cases_long_week_spatial = reduce(lambda left,right: pd.merge(left,right,on=[&#39;GEOID&#39;, &#39;state&#39;], how=&#39;left&#39;), data_frames) # filter out some states and subset data for only latest week US_cases_long_week_spatial = US_cases_long_week_spatial[(US_cases_long_week_spatial.GEOID &lt; 60) &amp; (US_cases_long_week_spatial.state != &quot;Alaska&quot;) &amp; (US_cases_long_week_spatial.state != &quot;Hawaii&quot;)] US_cases_long_week_spatial = US_cases_long_week_spatial[US_cases_long_week_spatial.week_of_year == max(US_cases_long_week_spatial.week_of_year)] print(US_cases_long_week_spatial.head(20)) ## REGION DIVISION ... cases_count_pos cases_rate_100K ## 44 3 5 ... 6519.0 351.809018 ## 89 3 5 ... 54246.0 288.522449 ## 134 2 3 ... 64085.0 499.468771 ## 179 2 4 ... 42815.0 807.232380 ## 224 3 5 ... 14245.0 246.728530 ## 269 1 1 ... 5953.0 565.569698 ## 314 4 8 ... 8823.0 562.841370 ## 359 1 1 ... 2882.0 218.918775 ## 404 3 5 ... 25141.0 263.657331 ## 449 1 1 ... 471.0 75.270759 ## 494 1 1 ... 11112.0 310.903705 ## 539 3 5 ... 3443.0 383.435754 ## 584 4 8 ... 13521.0 656.620915 ## 629 4 9 ... 100844.0 270.693400 ## 674 1 2 ... 28124.0 319.885567 ## 719 2 3 ... 32816.0 577.036764 ## 764 4 9 ... 8950.0 233.615952 ## 809 2 4 ... 12405.0 679.226935 ## 854 1 2 ... 47599.0 374.725081 ## 899 4 9 ... 18630.0 277.044973 ## ## [20 rows x 22 columns] Let’s create a choropleth map for the latest week’s COVID-19 cases using the Matplotlib module: # set the value column that will be visualised variable = &#39;cases_rate_100K&#39; # set the range for the choropleth values vmin, vmax = 0, 600 # create figure and axes for Matplotlib fig, ax = plt.subplots(1, figsize=(15, 15)) # remove the axis ax.axis(&#39;off&#39;) # add a title and annotation ## (0.0, 1.0, 0.0, 1.0) ax.set_title(&#39;COVID-19 cases rates for last week&#39;, fontdict={&#39;fontsize&#39;: &#39;15&#39;, &#39;fontweight&#39; : &#39;3&#39;}) ax.annotate(&#39;Data Sources: Harvard Dataverse, 2020; U.S. Census Bureau, 2019&#39;, xy=(0.6, .05), xycoords=&#39;figure fraction&#39;, fontsize=12, color=&#39;#555555&#39;) # create an axes on the right side of ax. The width of cax will be 3% of ax and the padding between cax and ax will be fixed at 0.05 inch. divider = make_axes_locatable(ax) cax = divider.append_axes(&quot;right&quot;, size=&quot;3%&quot;, pad=0.05) # Create colorbar legend legend = plt.cm.ScalarMappable(cmap=&#39;Greens&#39;, norm=plt.Normalize(vmin=vmin, vmax=vmax)) # empty array for the data range legend.set_array([]) # or alternatively sm._A = []. Not sure why this step is necessary, but many recommends it # add the colorbar to the figure fig.colorbar(legend, cax=cax) #fig.colorbar(legend) # create map ## &lt;matplotlib.colorbar.Colorbar object at 0x1847728d0&gt; US_cases_long_week_spatial.plot(column=variable, cmap=&#39;Greens&#39;, linewidth=0.8, ax=ax, cax=cax, edgecolor=&#39;1.0&#39;) plt.show() Interactive Maps Static maps are great for publications. Interactive maps, which can be viewed in a browser, can potentially provide a much richer source of information. Firstly, we need to create a function to get the polygon x and y coordinates: def getPolyCoords(row, geom, coord_type): &quot;&quot;&quot;Returns the coordinates (&#39;x|y&#39;) of edges/vertices of a Polygon/others&quot;&quot;&quot; # Parse the geometries and grab the coordinate geometry = row[geom] #print(geometry.type) if geometry.type==&#39;Polygon&#39;: if coord_type == &#39;x&#39;: # Get the x coordinates of the exterior # Interior is more complex: xxx.interiors[0].coords.xy[0] return list( geometry.exterior.coords.xy[0] ) elif coord_type == &#39;y&#39;: # Get the y coordinates of the exterior return list( geometry.exterior.coords.xy[1] ) if geometry.type in [&#39;Point&#39;, &#39;LineString&#39;]: if coord_type == &#39;x&#39;: return list( geometry.xy[0] ) elif coord_type == &#39;y&#39;: return list( geometry.xy[1] ) if geometry.type==&#39;MultiLineString&#39;: all_xy = [] for ea in geometry: if coord_type == &#39;x&#39;: all_xy.extend(list( ea.xy[0] )) elif coord_type == &#39;y&#39;: all_xy.extend(list( ea.xy[1] )) return all_xy if geometry.type==&#39;MultiPolygon&#39;: all_xy = [] for ea in geometry: if coord_type == &#39;x&#39;: all_xy.extend(list( ea.exterior.coords.xy[0] )) elif coord_type == &#39;y&#39;: all_xy.extend(list( ea.exterior.coords.xy[1] )) return all_xy else: # Finally, return empty list for unknown geometries return [] Now let’s use our function to get the Polygon x and y coordinates: # use our function to get the Polygon x and y coordinates US_cases_long_week_spatial[&#39;x&#39;] = US_cases_long_week_spatial.apply(getPolyCoords, geom=&#39;geometry&#39;, coord_type=&#39;x&#39;, axis=1) US_cases_long_week_spatial[&#39;y&#39;] = US_cases_long_week_spatial.apply(getPolyCoords, geom=&#39;geometry&#39;, coord_type=&#39;y&#39;, axis=1) # show only head of x and y columns print(US_cases_long_week_spatial[[&#39;x&#39;, &#39;y&#39;]].head(2)) # make a copy, drop the geometry column and create ColumnDataSource ## x y ## 44 [-81.747254, -81.746354, -81.746254, -81.74605... [39.095379, 39.096578, 39.096878, 39.096978, 3... ## 89 [-86.388646, -86.383854, -86.37907, -86.378965... [30.994180999999998, 30.994235999999997, 30.99... US_cases_long_week_spatial_df = US_cases_long_week_spatial.drop(&#39;geometry&#39;, axis=1).copy() gsource = ColumnDataSource(US_cases_long_week_spatial_df) In this section, we’ll focus on building a simple interactive map using the geopandas module. # create the color mapper color_mapper = LogColorMapper(palette=palette) # initialize our figure p = figure(title=&quot;COVID-19 cases rates for last week&quot;, plot_width=1200, plot_height=800) # Plot grid p.patches(&#39;x&#39;, &#39;y&#39;, source=gsource, fill_color={&#39;field&#39;: &#39;cases_rate_100K&#39;, &#39;transform&#39;: color_mapper}, fill_alpha=1.0, line_color=&quot;black&quot;, line_width=0.05) # create a color bar ## GlyphRenderer(id=&#39;1039&#39;, ...) color_bar = ColorBar(color_mapper=color_mapper, width=16, location=(0,0)) # add color bar to map p.add_layout(color_bar, &#39;right&#39;) # initialize our tool for interactivity to the map my_hover = HoverTool() # tell to the HoverTool that what information it should show to us my_hover.tooltips = [(&#39;name&#39;, &#39;@state&#39;), (&#39;cases rate&#39;, &#39;@cases_rate_100K&#39;)] # add this new tool into our current map p.add_tools(my_hover) # save the map output_file(&quot;data_py/covid-19_map_hover.html&quot;) save(p) # show the map ## &#39;/Users/sworthin/Documents/IQSS/datafest-project/data_py/covid-19_map_hover.html&#39; show(p) "],
["day-3-data-analysis-python.html", "DAY 3: Data analysis (Python) Descriptives Modeling", " DAY 3: Data analysis (Python) In this section, we will be exploring the relationships between COVID-19 cases and demographic data from the Census Bureau. If you did not complete the optional Census data section, you can still access these data by loading the following file: # load data US_cases_long_demogr_week = pd.read_csv(&#39;data_py/US_cases_long_demogr_week.csv&#39;) del US_cases_long_demogr_week[&quot;Unnamed: 0&quot;] print(US_cases_long_demogr_week.head(20)) ## state week_of_year ... cases_count_pos cases_rate_100K ## 0 Alabama 4 ... 0.0 0.000000 ## 1 Alabama 5 ... 0.0 0.000000 ## 2 Alabama 6 ... 0.0 0.000000 ## 3 Alabama 7 ... 0.0 0.000000 ## 4 Alabama 8 ... 0.0 0.000000 ## 5 Alabama 9 ... 0.0 0.000000 ## 6 Alabama 10 ... 0.0 0.000000 ## 7 Alabama 11 ... 23.0 0.469083 ## 8 Alabama 12 ... 134.0 2.732917 ## 9 Alabama 13 ... 673.0 13.725772 ## 10 Alabama 14 ... 1010.0 20.598856 ## 11 Alabama 15 ... 1743.0 35.548322 ## 12 Alabama 16 ... 1320.0 26.921277 ## 13 Alabama 17 ... 1518.0 30.959468 ## 14 Alabama 18 ... 1467.0 29.919328 ## 15 Alabama 19 ... 2001.0 40.810208 ## 16 Alabama 20 ... 1882.0 38.383214 ## 17 Alabama 21 ... 2707.0 55.209012 ## 18 Alabama 22 ... 3474.0 70.851905 ## 19 Alabama 23 ... 2548.0 51.966222 ## ## [20 rows x 11 columns] print(US_cases_long_demogr_week.shape) ## (2295, 11) Descriptives It’s always a good idea to start data analysis by looking at some descriptive statistics of the sample data. Here, we can inspect the demographic data we accessed through the Census API: US_cases_long_demogr_week_des = \\ US_cases_long_demogr_week.groupby([&#39;state&#39;], \\ as_index=False)[&#39;percent_age65over&#39;, &#39;percent_female&#39;, &#39;percent_white&#39;, &#39;percent_black&#39;].mean() print(US_cases_long_demogr_week_des) ## state ... percent_black ## 0 Alabama ... 26.784447 ## 1 Alaska ... 3.705582 ## 2 Arizona ... 5.179443 ## 3 Arkansas ... 15.675239 ## 4 California ... 6.460677 ## 5 Colorado ... 4.592657 ## 6 Connecticut ... 12.189341 ## 7 Delaware ... 23.162080 ## 8 District of Columbia ... 45.977253 ## 9 Florida ... 16.917588 ## 10 Georgia ... 32.570493 ## 11 Hawaii ... 2.186497 ## 12 Idaho ... 0.914684 ## 13 Illinois ... 14.619177 ## 14 Indiana ... 9.946141 ## 15 Iowa ... 4.060290 ## 16 Kansas ... 6.134766 ## 17 Kentucky ... 8.471502 ## 18 Louisiana ... 32.798463 ## 19 Maine ... 1.688052 ## 20 Maryland ... 31.074172 ## 21 Massachusetts ... 9.022876 ## 22 Michigan ... 14.097428 ## 23 Minnesota ... 7.013862 ## 24 Mississippi ... 37.785709 ## 25 Missouri ... 11.822200 ## 26 Montana ... 0.597786 ## 27 Nebraska ... 5.208984 ## 28 Nevada ... 10.269058 ## 29 New Hampshire ... 1.792366 ## 30 New Jersey ... 15.057739 ## 31 New Mexico ... 2.612135 ## 32 New York ... 17.586102 ## 33 North Carolina ... 22.221056 ## 34 North Dakota ... 3.409696 ## 35 Ohio ... 13.051219 ## 36 Oklahoma ... 7.779157 ## 37 Oregon ... 2.222519 ## 38 Pennsylvania ... 12.030099 ## 39 Rhode Island ... 8.505882 ## 40 South Carolina ... 26.958518 ## 41 South Dakota ... 2.298287 ## 42 Tennessee ... 17.051272 ## 43 Texas ... 12.895697 ## 44 Utah ... 1.482771 ## 45 Vermont ... 1.406115 ## 46 Virginia ... 19.880584 ## 47 Washington ... 4.358879 ## 48 West Virginia ... 3.605173 ## 49 Wisconsin ... 6.707556 ## 50 Wyoming ... 1.290174 ## ## [51 rows x 5 columns] Modeling The data we have consists of counts of COVID-19 cases over time for each of 50 U.S. states and D.C. These data will be challenging to model, since we will have to deal with the following issues: The response consists of counts with a huge number of zeros and an extended right tail. Typically, to model counts we’d use a poisson model. Here, the extended right tail suggests the data are overdispersed (i.e., the variance is greater than the mean), which would mean the restrictive assumptions of the poisson distribution are not met and may push us towards a quasi-poisson or negative binomial model. In addition, we may need some machinery in the model to deal with the excess of zeros (a zero-inflation component), since this is atypical for a poisson or negative binomial model. Let’s inspect the response variable: print(US_cases_long_demogr_week[&#39;cases_count_pos&#39;].describe()) ## count 2295.000000 ## mean 5822.301961 ## std 10866.637622 ## min 0.000000 ## 25% 207.000000 ## 50% 2058.000000 ## 75% 6230.000000 ## max 100844.000000 ## Name: cases_count_pos, dtype: float64 US_cases_long_demogr_week_filter = US_cases_long_demogr_week[US_cases_long_demogr_week.cases_count_pos &lt; 1000] sns.distplot(US_cases_long_demogr_week_filter[&#39;cases_count_pos&#39;], kde=False, color=&#39;red&#39;, bins=1000) plt.xlabel(&quot;$cases_count_pos$&quot;, fontsize=10) plt.ylabel(&quot;$count$&quot;, fontsize=10, rotation=90) plt.show() The data are inherently spatial in nature — in this case, at the state-level. The data are inherently temporal in nature — in this case, at the daily- or weekly-level. Cross-sectional models Let’s start with something at the simpler end of the scale. We can reduce complexity by initially modeling a single time point (for example, the most recent week of case data), with a subset of states, and just a single predictor — the intercept — to estimate the average number of cases. # filter the most recent week&#39;s data US_cases_latest_week = US_cases_long_demogr_week[US_cases_long_demogr_week.week_of_year == max(US_cases_long_demogr_week.week_of_year)] print(US_cases_latest_week.head(20)) ## state ... cases_rate_100K ## 44 Alabama ... 300.274210 ## 89 Alaska ... 583.286059 ## 134 Arizona ... 361.286199 ## 179 Arkansas ... 366.955574 ## 224 California ... 255.222289 ## 269 Colorado ... 534.162358 ## 314 Connecticut ... 311.671964 ## 359 Delaware ... 353.576431 ## 404 District of Columbia ... 183.776385 ## 449 Florida ... 252.568508 ## 494 Georgia ... 182.417146 ## 539 Hawaii ... 44.001153 ## 584 Idaho ... 493.714554 ## 629 Illinois ... 505.728419 ## 674 Indiana ... 564.538973 ## 719 Iowa ... 539.639374 ## 764 Kansas ... 612.601319 ## 809 Kentucky ... 423.979105 ## 854 Louisiana ... 259.271544 ## 899 Maine ... 85.477588 ## ## [20 rows x 11 columns] Now let’s inspect the response variable for just this last week of data: # histogram of last week&#39;s counts sns.distplot(US_cases_latest_week[&#39;cases_count_pos&#39;], kde=False, color=&#39;red&#39;, bins=50) plt.xlabel(&quot;$cases_count_pos$&quot;, fontsize=10) plt.ylabel(&quot;$count$&quot;, fontsize=10, rotation=90) plt.show() # distribution of cases in sample print(US_cases_latest_week[&#39;cases_count_pos&#39;].describe()) ## count 51.000000 ## mean 22147.000000 ## std 21029.232446 ## min 471.000000 ## 25% 7720.000000 ## 50% 16633.000000 ## 75% 27210.500000 ## max 100844.000000 ## Name: cases_count_pos, dtype: float64 Usually with count data, we’d fit a model designed to deal with the idiosyncrasies of counts — which are integer-only, lower bounded at zero, and generally heavily right skewed — such as a poisson, quasi-poisson, or negative binomial model. Here, however, the average number of counts is high and we don’t have any observations near the theoretical lower boundary of zero, so we can try a basic linear model since in this situation the Gaussian family of distributions approximates the poisson. # fit intercept-only OLS model US_cases_latest_week[&#39;intercept&#39;] = 1 Y = US_cases_latest_week[&#39;cases_count_pos&#39;] X = US_cases_latest_week[&#39;intercept&#39;] model_last_week1 = sm.OLS(Y,X) results = model_last_week1.fit() print(results.summary()) ## OLS Regression Results ## ============================================================================== ## Dep. Variable: cases_count_pos R-squared: 0.000 ## Model: OLS Adj. R-squared: 0.000 ## Method: Least Squares F-statistic: nan ## Date: Tue, 15 Dec 2020 Prob (F-statistic): nan ## Time: 21:35:05 Log-Likelihood: -579.50 ## No. Observations: 51 AIC: 1161. ## Df Residuals: 50 BIC: 1163. ## Df Model: 0 ## Covariance Type: nonrobust ## ============================================================================== ## coef std err t P&gt;|t| [0.025 0.975] ## ------------------------------------------------------------------------------ ## intercept 2.215e+04 2944.682 7.521 0.000 1.62e+04 2.81e+04 ## ============================================================================== ## Omnibus: 25.435 Durbin-Watson: 2.060 ## Prob(Omnibus): 0.000 Jarque-Bera (JB): 40.572 ## Skew: 1.664 Prob(JB): 1.55e-09 ## Kurtosis: 5.830 Cond. No. 1.00 ## ============================================================================== ## ## Notes: ## [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. print(results.conf_int(alpha=0.05)) ## 0 1 ## intercept 16232.433072 28061.566928 Let’s look at the model diagnostics: # model diagnostics fig, axs = plt.subplots(2, 2, squeeze=False, figsize=(6, 6)) fig.tight_layout() fig.delaxes(axs[1, 1]) axs[0,1].scatter(x=results.fittedvalues,y=results.resid,edgecolor=&#39;k&#39;) xmin = min(results.fittedvalues) xmax = max(results.fittedvalues) axs[0,1].hlines(y=0,xmin=xmin*0.9,xmax=xmax*1.1,color=&#39;red&#39;,linestyle=&#39;--&#39;,lw=3) axs[0,1].set_xlabel(&quot;Fitted values&quot;,fontsize=10) axs[0,1].set_ylabel(&quot;Residuals&quot;,fontsize=10) axs[0,1].set_title(&quot;Fitted vs. residuals plot&quot;,fontsize=10) stats.probplot(results.resid_pearson, plot=plt, fit=True) ## ((array([-2.21154155, -1.84175131, -1.62365924, -1.46329903, -1.33363779, ## -1.22318558, -1.1259265 , -1.03829303, -0.95798431, -0.88342315, ## -0.81347686, -0.74730127, -0.68424773, -0.62380483, -0.5655602 , ## -0.50917466, -0.45436405, -0.40088629, -0.34853176, -0.29711609, ## -0.24647455, -0.19645772, -0.14692788, -0.09775611, -0.0488197 , ## 0. , 0.0488197 , 0.09775611, 0.14692788, 0.19645772, ## 0.24647455, 0.29711609, 0.34853176, 0.40088629, 0.45436405, ## 0.50917466, 0.5655602 , 0.62380483, 0.68424773, 0.74730127, ## 0.81347686, 0.88342315, 0.95798431, 1.03829303, 1.1259265 , ## 1.22318558, 1.33363779, 1.46329903, 1.62365924, 1.84175131, ## 2.21154155]), array([-1.03075564, -1.02352761, -0.99851481, -0.99147699, -0.91610571, ## -0.88942856, -0.85024501, -0.84772471, -0.7700709 , -0.76902474, ## -0.76208202, -0.74315599, -0.72812929, -0.64396074, -0.63359421, ## -0.627555 , -0.58551828, -0.52655274, -0.52474573, -0.4799985 , ## -0.46325989, -0.41019091, -0.37576264, -0.35303238, -0.28926401, ## -0.26220643, -0.24351816, -0.20447727, -0.19178066, -0.19025897, ## -0.16724338, -0.15240689, -0.13214938, -0.04431926, 0.13357596, ## 0.14237324, 0.16724338, 0.19734434, 0.2842234 , 0.40962028, ## 0.50734139, 0.7541407 , 0.98282237, 1.085061 , 1.21031522, ## 1.26566674, 1.52639903, 1.94329489, 1.99427155, 2.47607706, ## 3.74226688])), (0.9321886416156772, 3.0402711516827844e-16, 0.9085011845308415)) axs[1,0].set_xlabel(&quot;Theoretical quantiles&quot;,fontsize=10) axs[1,0].set_ylabel(&quot;Sample quantiles&quot;,fontsize=10) axs[1,0].set_title(&quot;Q-Q plot of normalized residuals&quot;,fontsize=10) inf=influence(results) (c, p) = inf.cooks_distance axs[0,0].stem(np.arange(len(c)), c, markerfmt=&quot;,&quot;) ## &lt;StemContainer object of 3 artists&gt; axs[0,0].set_title(&quot;Cook&#39;s distance plot for the residuals&quot;,fontsize=10) plt.subplots_adjust(left=0.1, wspace=0.4, hspace=0.4) plt.show() We recovered the average number of cases for the latest week, pooled over all the states. Now we can try adding some of our explanatory variables. # fit OLS model with explanatory variables X = US_cases_latest_week[[&#39;percent_age65over&#39;, &#39;percent_female&#39;, &#39;percent_black&#39;]] Y = US_cases_latest_week[&#39;cases_count_pos&#39;] X = sm.add_constant(X) model_last_week2 = sm.OLS(Y,X) results2 = model_last_week2.fit() print(results2.summary()) ## OLS Regression Results ## ============================================================================== ## Dep. Variable: cases_count_pos R-squared: 0.094 ## Model: OLS Adj. R-squared: 0.036 ## Method: Least Squares F-statistic: 1.621 ## Date: Tue, 15 Dec 2020 Prob (F-statistic): 0.197 ## Time: 21:35:06 Log-Likelihood: -576.99 ## No. Observations: 51 AIC: 1162. ## Df Residuals: 47 BIC: 1170. ## Df Model: 3 ## Covariance Type: nonrobust ## ===================================================================================== ## coef std err t P&gt;|t| [0.025 0.975] ## ------------------------------------------------------------------------------------- ## const -4.659e+05 3.02e+05 -1.541 0.130 -1.07e+06 1.42e+05 ## percent_age65over -3865.7029 1836.319 -2.105 0.041 -7559.900 -171.505 ## percent_female 1.111e+04 6406.308 1.735 0.089 -1773.124 2.4e+04 ## percent_black -764.2939 504.074 -1.516 0.136 -1778.361 249.773 ## ============================================================================== ## Omnibus: 20.527 Durbin-Watson: 2.318 ## Prob(Omnibus): 0.000 Jarque-Bera (JB): 27.472 ## Skew: 1.462 Prob(JB): 1.08e-06 ## Kurtosis: 5.092 Cond. No. 5.72e+03 ## ============================================================================== ## ## Notes: ## [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. ## [2] The condition number is large, 5.72e+03. This might indicate that there are ## strong multicollinearity or other numerical problems. Let’s look at the model diagnostics: # model diagnostics fig, axs = plt.subplots(2, 2, squeeze=False, figsize=(6, 6)) fig.tight_layout() fig.delaxes(axs[1, 1]) axs[0,1].scatter(x=results2.fittedvalues,y=results2.resid,edgecolor=&#39;k&#39;) xmin = min(results2.fittedvalues) xmax = max(results2.fittedvalues) axs[0,1].hlines(y=0,xmin=xmin*0.9,xmax=xmax*1.1,color=&#39;red&#39;,linestyle=&#39;--&#39;,lw=3) axs[0,1].set_xlabel(&quot;Fitted values&quot;,fontsize=10) axs[0,1].set_ylabel(&quot;Residuals&quot;,fontsize=10) axs[0,1].set_title(&quot;Fitted vs. residuals plot&quot;,fontsize=10) stats.probplot(results2.resid_pearson, plot=plt, fit=True) ## ((array([-2.21154155, -1.84175131, -1.62365924, -1.46329903, -1.33363779, ## -1.22318558, -1.1259265 , -1.03829303, -0.95798431, -0.88342315, ## -0.81347686, -0.74730127, -0.68424773, -0.62380483, -0.5655602 , ## -0.50917466, -0.45436405, -0.40088629, -0.34853176, -0.29711609, ## -0.24647455, -0.19645772, -0.14692788, -0.09775611, -0.0488197 , ## 0. , 0.0488197 , 0.09775611, 0.14692788, 0.19645772, ## 0.24647455, 0.29711609, 0.34853176, 0.40088629, 0.45436405, ## 0.50917466, 0.5655602 , 0.62380483, 0.68424773, 0.74730127, ## 0.81347686, 0.88342315, 0.95798431, 1.03829303, 1.1259265 , ## 1.22318558, 1.33363779, 1.46329903, 1.62365924, 1.84175131, ## 2.21154155]), array([-1.65253149, -1.15171007, -1.15136638, -0.88745662, -0.83968806, ## -0.81798463, -0.77035518, -0.74767218, -0.70260376, -0.68411567, ## -0.66412683, -0.58215749, -0.55216123, -0.50982097, -0.50772736, ## -0.46892188, -0.44930698, -0.38593231, -0.37444205, -0.36790381, ## -0.35316036, -0.31657762, -0.31503715, -0.30046222, -0.29987997, ## -0.29341457, -0.28214219, -0.27893446, -0.27089349, -0.23078865, ## -0.22888131, -0.20298685, -0.17918033, -0.09309308, -0.02355087, ## 0.07163161, 0.07917909, 0.07978128, 0.25513112, 0.31931877, ## 0.51358473, 0.63018409, 0.88017482, 0.94426812, 1.36938364, ## 1.44044217, 1.85024983, 1.93863191, 1.95721227, 2.21881067, ## 3.38898395])), (0.9133672320998864, -3.5166749065205114e-16, 0.9181279129299906)) axs[1,0].set_xlabel(&quot;Theoretical quantiles&quot;,fontsize=10) axs[1,0].set_ylabel(&quot;Sample quantiles&quot;,fontsize=10) axs[1,0].set_title(&quot;Q-Q plot of normalized residuals&quot;,fontsize=10) inf=influence(results2) (c, p) = inf.cooks_distance axs[0,0].stem(np.arange(len(c)), c, markerfmt=&quot;,&quot;) ## &lt;StemContainer object of 3 artists&gt; axs[0,0].set_title(&quot;Cook&#39;s distance plot for the residuals&quot;,fontsize=10) plt.subplots_adjust(left=0.1, wspace=0.4, hspace=0.4) plt.show() We’re not able to detect any effects of interest here — perhaps because we’re only using one week of data. We actually have a year’s worth of data, so let’s try modeling this as a panel (a longitudinal dataset). Panel models We have case count data for each state, tracked at the weekly-level for a year. This means that the data are clustered at the state-level (i.e., observations within states are likely to be correlated with one another more than observations between different states). We could deal with this clustering in several different ways, but using a multi-level model with random intercepts grouped by state is a good, flexible option. Let’s start with a linear model. # linear mixed effects with random intercepts for states model_panel1 = smf.mixedlm(&quot;cases_count_pos ~ week_of_year + percent_age65over + percent_female + percent_black&quot;, US_cases_long_demogr_week, groups=&quot;state&quot;) model_panel1_results = model_panel1.fit(reml=False) print(model_panel1_results.summary()) ## Mixed Linear Model Regression Results ## =========================================================================== ## Model: MixedLM Dependent Variable: cases_count_pos ## No. Observations: 2295 Method: ML ## No. Groups: 51 Scale: 62845260.0324 ## Min. group size: 45 Log-Likelihood: -23942.2208 ## Max. group size: 45 Converged: Yes ## Mean group size: 45.0 ## --------------------------------------------------------------------------- ## Coef. Std.Err. z P&gt;|z| [0.025 0.975] ## --------------------------------------------------------------------------- ## Intercept -113674.192 84765.869 -1.341 0.180 -279812.243 52463.858 ## week_of_year 343.037 12.742 26.922 0.000 318.064 368.011 ## percent_age65over -960.246 514.813 -1.865 0.062 -1969.262 48.770 ## percent_female 2529.750 1796.013 1.409 0.159 -990.372 6049.872 ## percent_black -102.852 141.318 -0.728 0.467 -379.829 174.126 ## state Var 32111710.625 846.529 ## =========================================================================== print(model_panel1_results.conf_int(alpha=0.05, cols=None)) ## 0 1 ## Intercept -279812.243024 52463.858344 ## week_of_year 318.063804 368.010685 ## percent_age65over -1969.261801 48.769952 ## percent_female -990.371764 6049.871748 ## percent_black -379.829262 174.125712 ## state Var 0.301672 0.720257 Let’s look at the model diagnostics: # model diagnostics fig, axs = plt.subplots(2, 1, squeeze=False, figsize=(8, 8)) fig.tight_layout() axs[0,0].scatter(x=model_panel1_results.fittedvalues,y=model_panel1_results.resid,edgecolor=&#39;k&#39;) xmin = min(model_panel1_results.fittedvalues) xmax = max(model_panel1_results.fittedvalues) axs[0,0].hlines(y=0,xmin=xmin*0.9,xmax=xmax*1.1,color=&#39;red&#39;,linestyle=&#39;--&#39;,lw=3) axs[0,0].set_xlabel(&quot;Fitted values&quot;,fontsize=10) axs[0,0].set_ylabel(&quot;Residuals&quot;,fontsize=10) axs[0,0].set_title(&quot;Fitted vs. residuals plot&quot;,fontsize=10) stats.probplot(model_panel1_results.resid, plot=plt, fit=True) ## ((array([-3.4298304 , -3.18133149, -3.04364356, ..., 3.04364356, ## 3.18133149, 3.4298304 ]), array([-21724.68864953, -21565.7258941 , -21426.65140496, ..., ## 59822.63271488, 63227.66995944, 67010.32134171])), (6710.889431080678, 2.78148511566632e-11, 0.8545500284943002)) axs[1,0].set_xlabel(&quot;Theoretical quantiles&quot;,fontsize=10) axs[1,0].set_ylabel(&quot;Sample quantiles&quot;,fontsize=10) axs[1,0].set_title(&quot;Q-Q plot of residuals&quot;,fontsize=10) plt.subplots_adjust(left=0.12, hspace=0.25) plt.show() The model diagnostics look terrible here — why do you think that is? Now that we have a full year’s worth of data, for many states the earlier part of that year consisted of a very small number of cases — often zero cases. # provides a summary of the number of zeros print(US_cases_long_demogr_week[&#39;cases_count_pos&#39;].describe()) print(US_cases_long_demogr_week[&#39;cases_count_pos&#39;].value_counts()) count_total = sum(US_cases_long_demogr_week[&#39;cases_count_pos&#39;].value_counts().to_dict().values()) count_zero = US_cases_long_demogr_week[&#39;cases_count_pos&#39;].value_counts()[0.0] print(&quot;Count of zero is {}, about {:.4f} of the data.&quot;.format(count_zero, count_zero / count_total )) About 15% of the data are zeros. This makes the linear model a poor fit for these data. Let’s try a model designed specifically for count data — the poisson. To account for the fact that states have different population levels, we can include an exposure term using the offset argument to get counts per population unit: # Generalized Estimating Equations: poisson model poi=Poisson() ar=Autoregressive() gee_model1 = GEE.from_formula(&quot;cases_count_pos ~ week_of_year + percent_age65over + percent_female + percent_black&quot;, groups=&quot;state&quot;, \\ data=US_cases_long_demogr_week, \\ time=&#39;week_of_year&#39;, \\ cov_struct=ar, \\ family=poi, \\ offset=np.log(np.asarray(US_cases_long_demogr_week[&quot;pop_count_2019&quot;]))) gee_model1_results = gee_model1.fit(maxiter=200) ## /opt/anaconda3/envs/datafest/lib/python3.7/site-packages/statsmodels/genmod/generalized_estimating_equations.py:1252: IterationLimitWarning: Iteration limit reached prior to convergence ## IterationLimitWarning) print(gee_model1_results.summary()) ## GEE Regression Results ## =================================================================================== ## Dep. Variable: cases_count_pos No. Observations: 2295 ## Model: GEE No. clusters: 51 ## Method: Generalized Min. cluster size: 45 ## Estimating Equations Max. cluster size: 45 ## Family: Poisson Mean cluster size: 45.0 ## Dependence structure: Autoregressive Num. iterations: 200 ## Date: Tue, 15 Dec 2020 Scale: 1.000 ## Covariance type: robust Time: 21:35:59 ## ===================================================================================== ## coef std err z P&gt;|z| [0.025 0.975] ## ------------------------------------------------------------------------------------- ## Intercept 9.7617 6.996 1.395 0.163 -3.950 23.473 ## week_of_year 0.0714 0.006 12.025 0.000 0.060 0.083 ## percent_age65over 0.0511 0.025 2.071 0.038 0.003 0.099 ## percent_female -0.3960 0.145 -2.724 0.006 -0.681 -0.111 ## percent_black 0.0154 0.010 1.521 0.128 -0.004 0.035 ## ============================================================================== ## Skew: 1.9152 Kurtosis: 23.2141 ## Centered skew: 2.4730 Centered kurtosis: 22.7322 ## ============================================================================== print(ar.summary()) ## Autoregressive(1) dependence parameter: 0.768 print(&quot;scale=%.2f&quot; % (gee_model1_results.scale)) ## scale=1.00 We get a warning message: “IterationLimitWarning: Iteration limit reached prior to convergence”, but even if we specify a large value for maxiter (e.g., 2000) we still don’t achieve convergence. We could try specifying starting values, estimated using a simpler covariance structure, to try to get the estimating algorithm to converge. But, the lack of convergence may be a symptom of a more fundamental problem — that the data may not meet the restrictive assumptions of the poisson model (that the variance is equal to the mean). In which case, we may need a more flexible model. Let’s look at some model diagnostics anyway: # plot within-group residuals against time difference fig = gee_model1_results.plot_isotropic_dependence() plt.grid(True) plt.show() # plot mean-variance relationship without covariates yg = gee_model1.cluster_list(np.asarray(US_cases_long_demogr_week[&quot;cases_count_pos&quot;])) ymn = [x.mean() for x in yg] yva = [x.var() for x in yg] plt.grid(True) plt.plot(np.log(ymn), np.log(yva), &#39;o&#39;) plt.xlabel(&quot;Log Mean&quot;, size=13) plt.ylabel(&quot;Log Variance&quot;, size=13) plt.show() Overall, the poisson model does a much better job of capturing the idiosyncrasies of our data than the linear model. We can go further, however, by fitting a negative binomial model that can account for over- or under-dispersion (variance greater than or less than the mean). # Generalized Estimating Equations: negative binomial model nb = NegativeBinomial(alpha=1.) ar = Autoregressive() gee_model2 = GEE.from_formula(&quot;cases_count_pos ~ week_of_year + percent_age65over + percent_female + percent_black&quot;, groups=&quot;state&quot;, \\ data=US_cases_long_demogr_week, \\ time=&#39;week_of_year&#39;, \\ cov_struct=ar, \\ family=nb, \\ offset=np.log(np.asarray(US_cases_long_demogr_week[&quot;pop_count_2019&quot;]))) gee_model2_results = gee_model2.fit(maxiter=2000) print(gee_model2_results.summary()) ## GEE Regression Results ## =================================================================================== ## Dep. Variable: cases_count_pos No. Observations: 2295 ## Model: GEE No. clusters: 51 ## Method: Generalized Min. cluster size: 45 ## Estimating Equations Max. cluster size: 45 ## Family: NegativeBinomial Mean cluster size: 45.0 ## Dependence structure: Autoregressive Num. iterations: 52 ## Date: Tue, 15 Dec 2020 Scale: 1.000 ## Covariance type: robust Time: 21:36:13 ## ===================================================================================== ## coef std err z P&gt;|z| [0.025 0.975] ## ------------------------------------------------------------------------------------- ## Intercept 5.6206 4.490 1.252 0.211 -3.180 14.422 ## week_of_year 0.0745 0.005 15.051 0.000 0.065 0.084 ## percent_age65over 0.0321 0.019 1.707 0.088 -0.005 0.069 ## percent_female -0.3092 0.093 -3.324 0.001 -0.492 -0.127 ## percent_black 0.0135 0.007 2.042 0.041 0.001 0.026 ## ============================================================================== ## Skew: 1.3220 Kurtosis: 23.3385 ## Centered skew: 2.1260 Centered kurtosis: 22.5005 ## ============================================================================== print(ar.summary()) ## Autoregressive(1) dependence parameter: 0.761 print(&quot;scale=%.2f&quot; % (gee_model2_results.scale)) ## scale=1.00 Let’s look at some model diagnostics: # plot within-group residuals against time difference fig = gee_model2_results.plot_isotropic_dependence() plt.grid(True) plt.show() # plot mean-variance relationship without covariates yg = gee_model2.cluster_list(np.asarray(US_cases_long_demogr_week[&quot;cases_count_pos&quot;])) ymn = [x.mean() for x in yg] yva = [x.var() for x in yg] plt.grid(True) plt.plot(np.log(ymn), np.log(yva), &#39;o&#39;) plt.xlabel(&quot;Log Mean&quot;, size=13) plt.ylabel(&quot;Log Variance&quot;, size=13) plt.show() Let’s compare our last two models: # use quasi information criterion to compare poisson and negative binomial models print(gee_model2_results.qic()) ## (4709555305.822205, 6136.330127245823) print(gee_model1_results.qic()) ## (6283477749.315451, 8011349.269636027) Both count-based models improve upon the previous linear models, but are not satisfactory, probably because they cannot take account of the excessive zeros and they only use cluster-robust standard errors and thus cannot model how lower level coefficients vary across groups of the higher level. Python’s statsmodels has zero-inflated count model methods, but they cannot deal with panel/clustered data. However, we can fit generalized linear mixed effects models in a Bayesian framework using statsmodels. Initially, let’s try a model with random intercepts only: # Bayesian poisson mixed effects model with random intercepts for states formula = &quot;cases_count_pos ~ week_of_year + percent_age65over + percent_female + percent_black&quot; po_bay_panel1 = PoissonBayesMixedGLM.from_formula(formula, {&#39;state&#39;: &#39;0 + C(state)&#39;}, US_cases_long_demogr_week) po_bay_panel1_results = po_bay_panel1.fit_map() print(po_bay_panel1_results.summary()) ## Poisson Mixed GLM Results ## ================================================================= ## Type Post. Mean Post. SD SD SD (LB) SD (UB) ## ----------------------------------------------------------------- ## Intercept M 0.4348 1.2489 ## week_of_year M 0.0991 0.0000 ## percent_age65over M -0.5636 0.3065 ## percent_female M 0.2499 nan ## percent_black M -0.0271 0.1359 ## state V 2.4516 0.1852 11.606 8.014 16.810 ## ================================================================= ## Parameter types are mean structure (M) and variance structure (V) ## Variance parameters are modeled as log standard deviations We can also add random slopes for time to allow for different trajectories of case rates across states: # Bayesian poisson mixed effects model with independent random intercepts and slopes formula = &quot;cases_count_pos ~ week_of_year + percent_age65over + percent_female + percent_black&quot; po_bay_panel2 = PoissonBayesMixedGLM.from_formula(formula, {&#39;state&#39;: &#39;0 + C(state)&#39;, &quot;week_of_year&quot;: &#39;0 + C(week_of_year)&#39;}, US_cases_long_demogr_week) po_bay_panel2_results = po_bay_panel2.fit_map() print(po_bay_panel2_results.summary()) ## Poisson Mixed GLM Results ## ================================================================ ## Type Post. Mean Post. SD SD SD (LB) SD (UB) ## ---------------------------------------------------------------- ## Intercept M -0.2772 1.9277 ## week_of_year M 0.2134 0.0287 ## percent_age65over M -0.1699 0.0848 ## percent_female M 0.0765 0.0530 ## percent_black M 0.0158 0.0160 ## state V 0.0984 0.0985 1.103 0.906 1.344 ## week_of_year V 0.8998 0.1059 2.459 1.990 3.039 ## ================================================================ ## Parameter types are mean structure (M) and variance structure ## (V) ## Variance parameters are modeled as log standard deviations These models approximate the posterior distribution using variational inference, rather than sampling from it using Markov chain Monte Carlo, so unfortunately we can’t visualize parameter distributions. But, it seems safe to say that there is some variation among states in case rate trajectories over time. So far, we’ve only been modeling a linear trend for time. From our visualizations we know that this is unrealistic. How could we incorporate non-linear time elements in the model (e.g., splines, polynomials)? In addition, we haven’t dealt with the issue of zero-inflation — what are our options? We could try to fit our model in two stages: a logistic model for the zero/non-zero component. a truncated count model (poisson or negative binomial) for the positive count component only. "],
["day-4-data-archiving-python.html", "DAY 4: Data archiving (Python)", " DAY 4: Data archiving (Python) The data archiving session will probably be a GUI-based overview of using Harvard Dataverse. Though, we could put some demo code out there for how to create and populate a Dataverse entry using the API. "]
]
